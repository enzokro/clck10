<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>How to normalize spectrograms | Chris Kroenke’s blog</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="How to normalize spectrograms" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Scaling spectrograms for neural networks." />
<meta property="og:description" content="Scaling spectrograms for neural networks." />
<link rel="canonical" href="https://enzokro.dev/normalizing_spectrums/2020/08/04/Normalizing-spectrograms-for-deep-learning.html" />
<meta property="og:url" content="https://enzokro.dev/normalizing_spectrums/2020/08/04/Normalizing-spectrograms-for-deep-learning.html" />
<meta property="og:site_name" content="Chris Kroenke’s blog" />
<meta property="og:image" content="https://enzokro.dev/images/violin_spec.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-08-04T00:00:00-05:00" />
<script type="application/ld+json">
{"description":"Scaling spectrograms for neural networks.","mainEntityOfPage":{"@type":"WebPage","@id":"https://enzokro.dev/normalizing_spectrums/2020/08/04/Normalizing-spectrograms-for-deep-learning.html"},"@type":"BlogPosting","url":"https://enzokro.dev/normalizing_spectrums/2020/08/04/Normalizing-spectrograms-for-deep-learning.html","headline":"How to normalize spectrograms","dateModified":"2020-08-04T00:00:00-05:00","datePublished":"2020-08-04T00:00:00-05:00","image":"https://enzokro.dev/images/violin_spec.png","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://enzokro.dev/feed.xml" title="Chris Kroenke's blog" /><link rel="shortcut icon" type="image/x-icon" href="/images/favicon.ico"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>How to normalize spectrograms | Chris Kroenke’s blog</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="How to normalize spectrograms" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Scaling spectrograms for neural networks." />
<meta property="og:description" content="Scaling spectrograms for neural networks." />
<link rel="canonical" href="https://enzokro.dev/normalizing_spectrums/2020/08/04/Normalizing-spectrograms-for-deep-learning.html" />
<meta property="og:url" content="https://enzokro.dev/normalizing_spectrums/2020/08/04/Normalizing-spectrograms-for-deep-learning.html" />
<meta property="og:site_name" content="Chris Kroenke’s blog" />
<meta property="og:image" content="https://enzokro.dev/images/violin_spec.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-08-04T00:00:00-05:00" />
<script type="application/ld+json">
{"description":"Scaling spectrograms for neural networks.","mainEntityOfPage":{"@type":"WebPage","@id":"https://enzokro.dev/normalizing_spectrums/2020/08/04/Normalizing-spectrograms-for-deep-learning.html"},"@type":"BlogPosting","url":"https://enzokro.dev/normalizing_spectrums/2020/08/04/Normalizing-spectrograms-for-deep-learning.html","headline":"How to normalize spectrograms","dateModified":"2020-08-04T00:00:00-05:00","datePublished":"2020-08-04T00:00:00-05:00","image":"https://enzokro.dev/images/violin_spec.png","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

<link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
<link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css"><link type="application/atom+xml" rel="alternate" href="https://enzokro.dev/feed.xml" title="Chris Kroenke's blog" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"> </script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head><body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Chris Kroenke&#39;s blog</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a><a class="page-link" href="/search/">Search</a><a class="page-link" href="/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">How to normalize spectrograms</h1><p class="page-description">Scaling spectrograms for neural networks.</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2020-08-04T00:00:00-05:00" itemprop="datePublished">
        Aug 4, 2020
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      19 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/categories/#normalizing_spectrums">normalizing_spectrums</a>
        
      
      </p>
    

    
      
        <div class="pb-5 d-flex flex-wrap flex-justify-end">
          <div class="px-2">

    <a href="https://github.com/enzokro/clck10/tree/master/_notebooks/2020-08-04-Normalizing-spectrograms-for-deep-learning.ipynb" role="button" target="_blank">
<img class="notebook-badge-image" src="/assets/badges/github.svg" alt="View On GitHub">
    </a>
</div>

          <div class="px-2">
    <a href="https://mybinder.org/v2/gh/enzokro/clck10/master?filepath=_notebooks%2F2020-08-04-Normalizing-spectrograms-for-deep-learning.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/assets/badges/binder.svg" alt="Open In Binder"/>
    </a>
</div>

          <div class="px-2">
    <a href="https://colab.research.google.com/github/enzokro/clck10/blob/master/_notebooks/2020-08-04-Normalizing-spectrograms-for-deep-learning.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>
        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h1"><a href="#Introduction">Introduction </a></li>
<li class="toc-entry toc-h1"><a href="#Turning-audio-into-an-image">Turning audio into an image </a></li>
<li class="toc-entry toc-h1"><a href="#Sample-audio-dataset">Sample audio dataset </a>
<ul>
<li class="toc-entry toc-h2"><a href="#Importing-fastai-and-fastaudio-modules">Importing fastai and fastaudio modules </a></li>
<li class="toc-entry toc-h2"><a href="#Downloading-the-ESC-50-dataset">Downloading the ESC-50 dataset </a></li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#Normalizing-waveforms">Normalizing waveforms </a></li>
<li class="toc-entry toc-h1"><a href="#Extracting-spectrograms-from-audio">Extracting spectrograms from audio </a></li>
<li class="toc-entry toc-h1"><a href="#How-do-we-normalize-spectrograms?">How do we normalize spectrograms? </a></li>
<li class="toc-entry toc-h1"><a href="#Spectrogram-Normalization">Spectrogram Normalization </a>
<ul>
<li class="toc-entry toc-h2"><a href="#Building-the-dataset-loader">Building the dataset loader </a></li>
<li class="toc-entry toc-h2"><a href="#Finding-spectrogram-normalization-statistics">Finding spectrogram normalization statistics </a></li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#Making-Normalization-Transforms">Making Normalization Transforms </a></li>
<li class="toc-entry toc-h1"><a href="#Training-with-different-statistics">Training with different statistics </a>
<ul>
<li class="toc-entry toc-h2"><a href="#Finding-a-good-learning-rate.">Finding a good learning rate. </a></li>
<li class="toc-entry toc-h2"><a href="#Training-helpers">Training helpers </a></li>
<li class="toc-entry toc-h2"><a href="#Baseline-performance">Baseline performance </a></li>
<li class="toc-entry toc-h2"><a href="#Performance-with-global-statistics">Performance with global statistics </a></li>
<li class="toc-entry toc-h2"><a href="#Performance-with-channel-statistics">Performance with channel statistics </a></li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#Conclusions">Conclusions </a></li>
<li class="toc-entry toc-h1"><a href="#For-fun,-going-as-high-as-we-can-based-on-ImageNette">For fun, going as high as we can based on ImageNette </a></li>
</ul><!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-08-04-Normalizing-spectrograms-for-deep-learning.ipynb
-->

<div class="container" id="notebook-container">
        
    
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Introduction">
<a class="anchor" href="#Introduction" aria-hidden="true"><span class="octicon octicon-link"></span></a>Introduction<a class="anchor-link" href="#Introduction"> </a>
</h1>
<p>Spectrograms are often treated as images in deep learning to leverage the many great techniques from image classification. A spectrogram, however, is fundamentally different from natural images in several ways as we will see below. That raises the central question of this post: what is the proper way to normalize spectrograms when training neural networks?</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Turning-audio-into-an-image">
<a class="anchor" href="#Turning-audio-into-an-image" aria-hidden="true"><span class="octicon octicon-link"></span></a>Turning audio into an image<a class="anchor-link" href="#Turning-audio-into-an-image"> </a>
</h1>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>It is hard to overstate the success of deep neural networks for classifying images. This challenging task was previously dominated by expert handcrafted features. Now, the features are automatically learned from labeled data instead. The performance of these learned features completely shifted the Computer Vision paradigm. We would ideally like to follow these same, proven approaches in audio tasks.</p>
<p>However, audio is usually treated as a one dimensional signal in Machine Learning applications. Even stereo recordings with more than one channel are first mixed into mono (single channel) before processing. That means raw audio is unusable with 2D CNNs which are the bread and butter of modern image recognition. If we could represent audio in two dimensions, like an image, it opens up a host of deep learning image classification techniques.</p>
<p>Thankfully there are many ways to transform audio into two dimensions. The most common one is the short-time Fourier Transform <a href="https://www.dsprelated.com/freebooks/sasp/Short_Time_Fourier_Transform.html">(STFT)</a>. The STFT turns audio into a spectrogram: a 2-D signal representation in time and frequency. Since a spectrogram is two dimensional we can treat it like an image!</p>
<p>Before plugging spectrograms into a neural network we need a data pipeline. Data normalization is one of the first steps in any good data pipeline. This is because learning is difficult with unnormalized inputs and convergence could take a very long time. For natural images, normalization involves:</p>
<ul>
<li>Centering the image values by subtracting a single, scalar mean. </li>
<li>Dividing the images by a scalar standard deviation to give them a variance of 1.  </li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Sample-audio-dataset">
<a class="anchor" href="#Sample-audio-dataset" aria-hidden="true"><span class="octicon octicon-link"></span></a>Sample audio dataset<a class="anchor-link" href="#Sample-audio-dataset"> </a>
</h1>
<p>To keep things practical, we apply these normalization techniques to a <a href="https://github.com/fastaudio/Audio-Competition">sound classification challenge</a> hosted by <code>fastaudio</code>. <code>fastaudio</code> is a community extension of the <code>fastai</code> library to make neural network audio tasks more accessible.<br>
The challenge here is to classify sounds in the <a href="https://github.com/karolpiczak/ESC-50">ESC-50 dataset</a>.
ESC-50 stands for "Environment Sound Classification with 50 classes". This set has a diverse set of sounds which gives a feel for how different audio spectrograms can be. Every file is five seconds long which makes batch processing easier since the samples are of the same size.</p>
<p>Many lines below are based on the <code>fastaudio</code> <a href="https://github.com/fastaudio/Audio-Competition/blob/master/ESC-50-baseline-1Fold.ipynb">baseline results notebook</a>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Importing-fastai-and-fastaudio-modules">
<a class="anchor" href="#Importing-fastai-and-fastaudio-modules" aria-hidden="true"><span class="octicon octicon-link"></span></a>Importing <code>fastai</code> and <code>fastaudio</code> modules<a class="anchor-link" href="#Importing-fastai-and-fastaudio-modules"> </a>
</h2>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">fastai.vision.all</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">fastaudio.core.all</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">fastaudio.augment.all</span> <span class="kn">import</span> <span class="o">*</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Downloading-the-ESC-50-dataset">
<a class="anchor" href="#Downloading-the-ESC-50-dataset" aria-hidden="true"><span class="octicon octicon-link"></span></a>Downloading the ESC-50 dataset<a class="anchor-link" href="#Downloading-the-ESC-50-dataset"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The first step is downloading the ESC-50 dataset. It is included in <code>fastaudio</code> so we grab it with fastai's <code>untar_data</code>.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># already in fastaudio, can download with fastai's `untar_data`</span>
<span class="n">path</span> <span class="o">=</span> <span class="n">untar_data</span><span class="p">(</span><span class="n">URLs</span><span class="o">.</span><span class="n">ESC50</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Looking inside of the downloaded <code>audio</code> folder reveals many <code>.wav</code> files.<br>
Below we view the files with the <code>ls</code> method, a handy fastai addition to python's standard <code>pathlib.Path</code> class.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">wavs</span> <span class="o">=</span> <span class="p">(</span><span class="n">path</span><span class="o">/</span><span class="s2">"audio"</span><span class="p">)</span><span class="o">.</span><span class="n">ls</span><span class="p">()</span>
<span class="n">wavs</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The output of <code>ls</code> shows there are 2,000 audio files. But the filenames are not too descriptive, so how can we know what is actually in each one?<br>
As with many datasets, the download includes a table with more information about the data (aka metadata).</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># read the audio metadata and show its first few rows</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">path</span><span class="o">/</span><span class="s2">"meta"</span><span class="o">/</span><span class="s2">"esc50.csv"</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The key information from the table is in the <code>filename</code> and <code>category</code> columns. The <code>filename</code> gives the name of the file inside of the <code>audio</code> folder. The <code>category</code> tells us which class it belongs to.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We pick the last file in the data directory as our working example. The file's <code>name</code> can index into the metadata table above to display its specific information.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># pick the row where "filename" matches the waveform's "name".</span>
<span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">df</span><span class="o">.</span><span class="n">filename</span> <span class="o">==</span> <span class="n">wavs</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">name</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This is a recording of crickets! We can load this audio file using the <code>create</code> function of the <code>AudioTensor</code> class in <code>fastaudio</code>. <code>AudioTensor</code> wraps a <code>torch.Tensor</code> with some extra syntactic sugar.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># create an AudioTensor from a file path</span>
<span class="n">sample</span> <span class="o">=</span> <span class="n">AudioTensor</span><span class="o">.</span><span class="n">create</span><span class="p">(</span><span class="n">wavs</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Thanks to <code>AudioTensor</code> we can directly plot and even listen to our sample. Each "burst" in the file is a cricket chirp. There are three full chirps and the early starts of a fourth chirp.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">sample</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Normalizing-waveforms">
<a class="anchor" href="#Normalizing-waveforms" aria-hidden="true"><span class="octicon octicon-link"></span></a>Normalizing waveforms<a class="anchor-link" href="#Normalizing-waveforms"> </a>
</h1>
<p>The first step is normalizing the audio waveform itself. We give it a mean of zero and unit variance in the typical way:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>
$$\text{norm_audio} = \frac{\text{audio} - mean(\text{audio})}{std(\text{audio})} $$
</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># normalize the waveform</span>
<span class="n">norm_sample</span> <span class="o">=</span> <span class="p">(</span><span class="n">sample</span> <span class="o">-</span> <span class="n">sample</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span> <span class="o">/</span> <span class="n">sample</span><span class="o">.</span><span class="n">std</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's check if the mean is roughly 0 and the variance is roughly one:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># checking if normalization worked</span>
<span class="n">norm_sample</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span><span class="n">norm_sample</span><span class="o">.</span><span class="n">var</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Success! The waveform is normalized.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Extracting-spectrograms-from-audio">
<a class="anchor" href="#Extracting-spectrograms-from-audio" aria-hidden="true"><span class="octicon octicon-link"></span></a>Extracting spectrograms from audio<a class="anchor-link" href="#Extracting-spectrograms-from-audio"> </a>
</h1>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We are now ready to extract a spectrogram from the normalized audio. The <code>fastaudio</code> library wraps parts of <code>torchaudio</code> to convert an <code>AudioTensor</code> into a spectrogram.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># create a fastaudio Transform that converts audio into spectrograms</span>
<span class="n">cfg</span> <span class="o">=</span> <span class="n">AudioConfig</span><span class="o">.</span><span class="n">BasicSpectrogram</span><span class="p">()</span>
<span class="n">audio2spec</span> <span class="o">=</span> <span class="n">AudioToSpec</span><span class="o">.</span><span class="n">from_cfg</span><span class="p">(</span><span class="n">cfg</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The spectrogram computation details are not important here. But a quick look at the <a href="https://pytorch.org/audio/_modules/torchaudio/functional.html#spectrogram">spectrogram source code</a> shows that it boils down to some pre and post processing around a <a href="https://pytorch.org/docs/stable/generated/torch.stft.html">torch.stft</a> call. We can now transform our audio into a spectrogram.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># extract the spectrogram</span>
<span class="n">spec</span> <span class="o">=</span> <span class="n">audio2spec</span><span class="p">(</span><span class="n">norm_sample</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Just like with the waveform, <code>fastaudio</code> can directly plot spectrograms. The colorbar on the right is especially helpful here, since <code>matplotlib</code> always normalizes the values in a plot to a certain color range. Withtout the colorbar, the fixed color range makes it impossible to know or even guess the exact values in a spectrogram plot.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">spec</span><span class="o">.</span><span class="n">show</span><span class="p">();</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This is a good time to compare the shapes of the audio vs. the spectrogram to see the added dimension that makes the spectrogram an "image".</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Audio shape [channels, samples]: </span><span class="si">{</span><span class="n">norm_sample</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Spectrum shape [channels, bins, time_steps]: </span><span class="si">{</span><span class="n">spec</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="How-do-we-normalize-spectrograms?">
<a class="anchor" href="#How-do-we-normalize-spectrograms?" aria-hidden="true"><span class="octicon octicon-link"></span></a>How do we normalize spectrograms?<a class="anchor-link" href="#How-do-we-normalize-spectrograms?"> </a>
</h1>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>As stated in the introduction, a spectrogram is fundamentally different from an image.</p>
<p>Both dimensions in an image are in the spatial domain and have the same units.
Images are stored as integers in the range of <code>[0, 255]</code>.<br>
To normalize first divide all pixels by 255, the max possible value, to map them into <code>[0, 1]</code>.<br>
Then, find the statistics that approximately center the data and give it unit variance.<br>
For a color image we have three channels (RGB) and normalize each one.
If the image is grayscale then we normalize its single channel instead.
Given the layout of natural images, and the fact that both dimensions are in the same domain, it makes sense to normalize each channel with single, global values.</p>
<p>Spectrograms are different.
In a spectrogram, one dimension represents time and the other represents frequency.
Different quantities, scales, and sizes.
Frequency dimension given by choice of FFT size. Sets our spectral resolution.
Time dimension given by length of our signal, FFT size, and window overlap. Sets our temporal resolution.
In fact what we call the spectrogram is actually the log of the power spectrum.
Below we give a quick recap of how the spectrogram is computed to show how it differs from images.</p>
<p>If $\text{x}$ is the input audio then the STFT returns the spectrum:

$$\text{spectrum} = \text{STFT(x)}$$

We are more interested in the energy or power of the signal, so we take the absolute value of the STFT and square it:<br>
$$\text{power_spectrum} = |\text{STFT(x)}|^2$$ 
While we could use the power spectrum as our input "image", it is a bit problematic. The power spectrum often has a few, strong peaks and many small values. This means the values are <a href="https://danielsdiscoveries.wordpress.com/2017/09/29/spectrogram-input-normalisation-for-neural-networks/">heavy-tailed</a> and make for poor network inputs.<br>
To deal with this we take the log of the power spectrum which spreads out the values. This becomes the spectrogram:

$$\text{spectrogram} = log(|\text{STFT(x)}|^2)$$

The range of the log function is from $-\infty$ to $+\infty$ which is very different than the integers from 0 to 255.</p>
<p>However, the spectrogram also introduces the notion of a different type of channel.
This makes "channel" an overloaded term for our purposes but it is still a crucial piece of the puzzle. 
The spectrogram transform can be interpreted as a "channelizer".
That is a fancy way to say that it takes the continuous frequency spectrum of our signal and chops it up into discrete bins, or channels. For example, consider a signal sampled at 16 kHz (typical for audio) where we take an STFT of size 512. Our spectrogram will have 512 channels where each one has a "bandwidth" of $$16 \ \text{kHz} \ \ / \ \ 512 \ \text{bins} = 31.25 \ \text{Hz per bin}$$</p>
<p>Even though these spectrum channels are different from the channels in an image, it raises the question: should we (or can we?) normalize an entire spectrum "image" with a single, global value? Or do we need to normalize each channel, as is done with images?</p>
<p>There is no clear answer here, and your approach will likely depend both on the specifics of your problem and where your system will be deployed.
For example, if your are building a system that will be deployed in a similar environment as the training one, then it might make more sense to normalize by channels.
Your channel-based normalization statistics will follow the average noise floor and activity of the training data.
This motivation hold if you expect roughly the same patterns and distributions of activity once the system is deployed.
However, it will be critical to monitor the deployed environment and update the statistics as needed, else you slowly shift out of domain.</p>
<p>If your system will instead be used in a completely different environment, of which you have no knowledge, then the global statistics could be a better fit. While not as technically sound, your model won't be as surprised by radically new activity across different channels.</p>
<p>Lastly, we also have issue of Transfer Learning. In Transfer Learning it is best-practice to normalize the new dataset with the statistics from the old dataset. In most cases that means normalizing with ImageNet statistics.
So if you are doing transfer learning, the easiest approach will be to use original stats. 
If your dataset is large enough that you are training from scratch, then the above applies.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Spectrogram-Normalization">
<a class="anchor" href="#Spectrogram-Normalization" aria-hidden="true"><span class="octicon octicon-link"></span></a>Spectrogram Normalization<a class="anchor-link" href="#Spectrogram-Normalization"> </a>
</h1>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Normalization statistics always come from the training set (this is a crucial place to avoid data leakage from the validation set). That means stepping through the training set once and getting a mean and standard deviation from each mini-batch. Then, we average the statistics from each mini-batch to get a pair of "global" statistics.<br>
One small detail: if your training dataset is large enough, you often do not need to iterate through the entire set. It is enough to sample 10% to 20% of the dataset for accurate statistics. However, since ESC-50 is small enough we get statistics from the whole set.</p>
<p>First we to accumulate these statistics over mini-batches. We can borrow and slightly refactor a class from the incredibly helpful guide <a href="http://notmatthancock.github.io/2017/03/23/simple-batch-stat-updates.html">here</a>.<br>
The <code>StatsRecorder</code> class below tracks the mean and standard deviation across mini-batches.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">StatsRecorder</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">red_dims</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">)):</span>
        <span class="sd">"""Accumulates normalization statistics across mini-batches.</span>
<span class="sd">        ref: http://notmatthancock.github.io/2017/03/23/simple-batch-stat-updates.html</span>
<span class="sd">        """</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">red_dims</span> <span class="o">=</span> <span class="n">red_dims</span> <span class="c1"># which mini-batch dimensions to average over</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">nobservations</span> <span class="o">=</span> <span class="mi">0</span>   <span class="c1"># running number of observations</span>

    <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
        <span class="sd">"""</span>
<span class="sd">        data: ndarray, shape (nobservations, ndimensions)</span>
<span class="sd">        """</span>
        <span class="c1"># initialize stats and dimensions on first batch</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">nobservations</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">mean</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">red_dims</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">std</span>  <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">std</span> <span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">red_dims</span><span class="p">,</span><span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">nobservations</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">ndimensions</span>   <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ndimensions</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">'Data dims do not match previous observations.'</span><span class="p">)</span>
            
            <span class="c1"># find mean of new mini batch</span>
            <span class="n">newmean</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">red_dims</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="n">newstd</span>  <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">red_dims</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            
            <span class="c1"># update number of observations</span>
            <span class="n">m</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">nobservations</span> <span class="o">*</span> <span class="mf">1.0</span>
            <span class="n">n</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

            <span class="c1"># update running statistics</span>
            <span class="n">tmp</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mean</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">mean</span> <span class="o">=</span> <span class="n">m</span><span class="o">/</span><span class="p">(</span><span class="n">m</span><span class="o">+</span><span class="n">n</span><span class="p">)</span><span class="o">*</span><span class="n">tmp</span> <span class="o">+</span> <span class="n">n</span><span class="o">/</span><span class="p">(</span><span class="n">m</span><span class="o">+</span><span class="n">n</span><span class="p">)</span><span class="o">*</span><span class="n">newmean</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">std</span>  <span class="o">=</span> <span class="n">m</span><span class="o">/</span><span class="p">(</span><span class="n">m</span><span class="o">+</span><span class="n">n</span><span class="p">)</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">std</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">n</span><span class="o">/</span><span class="p">(</span><span class="n">m</span><span class="o">+</span><span class="n">n</span><span class="p">)</span><span class="o">*</span><span class="n">newstd</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span>\
                        <span class="n">m</span><span class="o">*</span><span class="n">n</span><span class="o">/</span><span class="p">(</span><span class="n">m</span><span class="o">+</span><span class="n">n</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">tmp</span> <span class="o">-</span> <span class="n">newmean</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">std</span>  <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">std</span><span class="p">)</span>
                                 
            <span class="c1"># update total number of seen samples</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">nobservations</span> <span class="o">+=</span> <span class="n">n</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>By default <code>StatsRecorder</code> averages over the image channel dimensions (grayscale or RGB). The <code>red_dims</code> argument might look familiar from normalization code in other Computer Vision tasks (even the <code>Normalize</code> in <code>fastai</code>).<br>
Averaging instead over spectrogram channels is as easy as passing a different <code>red_dims</code>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Building-the-dataset-loader">
<a class="anchor" href="#Building-the-dataset-loader" aria-hidden="true"><span class="octicon octicon-link"></span></a>Building the dataset loader<a class="anchor-link" href="#Building-the-dataset-loader"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Next we need to step through the training dataset. The setup belows follows the <code>fastaudio</code> ESC-50 baseline. One thing to mention is that by default, <code>fastaudio</code> resamples audio to 16 kHz. While much of the audio in the wild is sampled at 16 kHz, ESC-50 is actually sampled at a higher 44.1 kHz rate. Downsampling risks throwing away some information. But, keeping the higher sampling rate almost triples the "width" of the spectrogram. This much larger image potentially limits our batch size and architecture choices. For now we stick with downsampling to 16 kHz since it yields a very reasonable spectrogram shape of [201, 401].</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We also need a <code>Transform</code> that normalizes individual audio waveforms.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">AudioNormalize</span><span class="p">(</span><span class="n">Transform</span><span class="p">):</span>
    <span class="s2">"Normalizes a single `AudioTensor`."</span>
    <span class="k">def</span> <span class="nf">encodes</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span><span class="n">AudioTensor</span><span class="p">):</span> <span class="k">return</span> <span class="p">(</span><span class="n">x</span><span class="o">-</span><span class="n">x</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span> <span class="o">/</span> <span class="n">x</span><span class="o">.</span><span class="n">std</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">CrossValidationSplitter</span><span class="p">(</span><span class="n">col</span><span class="o">=</span><span class="s1">'fold'</span><span class="p">,</span> <span class="n">fold</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="s2">"Split `items` (supposed to be a dataframe) by fold in `col`"</span>
    <span class="k">def</span> <span class="nf">_inner</span><span class="p">(</span><span class="n">o</span><span class="p">):</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">o</span><span class="p">,</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">),</span> <span class="s2">"ColSplitter only works when your items are a pandas DataFrame"</span>
        <span class="n">col_values</span> <span class="o">=</span> <span class="n">o</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="n">col</span><span class="p">]</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">col</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="k">else</span> <span class="n">o</span><span class="p">[</span><span class="n">col</span><span class="p">]</span>
        <span class="n">valid_idx</span> <span class="o">=</span> <span class="p">(</span><span class="n">col_values</span> <span class="o">==</span> <span class="n">fold</span><span class="p">)</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">'bool'</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">IndexSplitter</span><span class="p">(</span><span class="n">mask2idxs</span><span class="p">(</span><span class="n">valid_idx</span><span class="p">))(</span><span class="n">o</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_inner</span>

<span class="c1"># do not resample audio</span>
<span class="n">auds</span> <span class="o">=</span> <span class="n">DataBlock</span><span class="p">(</span><span class="n">blocks</span><span class="o">=</span><span class="p">(</span><span class="n">AudioBlock</span><span class="p">,</span> <span class="n">CategoryBlock</span><span class="p">),</span>  
                 <span class="n">get_x</span><span class="o">=</span><span class="n">ColReader</span><span class="p">(</span><span class="s2">"filename"</span><span class="p">,</span> <span class="n">pref</span><span class="o">=</span><span class="n">path</span><span class="o">/</span><span class="s2">"audio"</span><span class="p">),</span> 
                 <span class="n">splitter</span><span class="o">=</span><span class="n">CrossValidationSplitter</span><span class="p">(</span><span class="n">fold</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
                 <span class="n">item_tfms</span> <span class="o">=</span> <span class="p">[</span><span class="n">AudioNormalize</span><span class="p">],</span>
                 <span class="n">batch_tfms</span> <span class="o">=</span> <span class="p">[</span><span class="n">audio2spec</span><span class="p">],</span>
                 <span class="n">get_y</span><span class="o">=</span><span class="n">ColReader</span><span class="p">(</span><span class="s2">"category"</span><span class="p">))</span>
<span class="n">dbunch</span> <span class="o">=</span> <span class="n">auds</span><span class="o">.</span><span class="n">dataloaders</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">bs</span><span class="o">=</span><span class="mi">64</span><span class="p">)</span>
<span class="n">dbunch</span><span class="o">.</span><span class="n">show_batch</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span><span class="mi">7</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Finding-spectrogram-normalization-statistics">
<a class="anchor" href="#Finding-spectrogram-normalization-statistics" aria-hidden="true"><span class="octicon octicon-link"></span></a>Finding spectrogram normalization statistics<a class="anchor-link" href="#Finding-spectrogram-normalization-statistics"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Below we create two statistic recorders: one for global statistics and one for channel-based statistics. Then we step through the entire dataset and find the normalization stats.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># create recorders</span>
<span class="n">global_stats</span>  <span class="o">=</span> <span class="n">StatsRecorder</span><span class="p">()</span>
<span class="n">channel_stats</span> <span class="o">=</span> <span class="n">StatsRecorder</span><span class="p">(</span><span class="n">red_dims</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>

<span class="c1"># step through the dataset</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">idx</span><span class="p">,(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">dbunch</span><span class="o">.</span><span class="n">train</span><span class="p">)):</span>
        <span class="c1"># update normalization statistics</span>
        <span class="n">global_stats</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">channel_stats</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    
<span class="c1"># parse out the stats</span>
<span class="n">global_mean</span><span class="p">,</span><span class="n">global_std</span> <span class="o">=</span> <span class="n">global_stats</span><span class="o">.</span><span class="n">mean</span><span class="p">,</span><span class="n">global_stats</span><span class="o">.</span><span class="n">std</span>
<span class="n">channel_mean</span><span class="p">,</span><span class="n">channel_std</span> <span class="o">=</span> <span class="n">channel_stats</span><span class="o">.</span><span class="n">mean</span><span class="p">,</span><span class="n">channel_stats</span><span class="o">.</span><span class="n">std</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We can check the shape of both statistics to make sure they are as expected. For the global "grayscale" statistics, we expect a shape of: <code>[1,1,1,1]</code>. With spectrogram channel normalizations, we expect a shape of <code>[1,1,201,1]</code> with one normalization stat for each spectrogram bin.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">'Shapes of global mean/std:'</span><span class="p">)</span>
<span class="n">global_mean</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span><span class="n">global_std</span><span class="o">.</span><span class="n">shape</span> 
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">'Shapes of channel mean/std:'</span><span class="p">)</span>
<span class="n">channel_mean</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span><span class="n">channel_mean</span><span class="o">.</span><span class="n">shape</span> 
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Making-Normalization-Transforms">
<a class="anchor" href="#Making-Normalization-Transforms" aria-hidden="true"><span class="octicon octicon-link"></span></a>Making Normalization <code>Transforms</code><a class="anchor-link" href="#Making-Normalization-Transforms"> </a>
</h1>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We need to extend the <code>Normalize</code> in <code>fastai</code> to normalize the spectrogram mini-batches. The reason is type dispatch. <code>fastai</code> normalization uses ImageNet statistics due to the focus on transfer learning with color images. But this ImageNet normalization is only defined for the <code>TensorImage</code> class, while <code>AudioSpectrogram</code> subclasses the different <code>TensorImageBase</code>. The solution is to define <code>encodes</code> and <code>decodes</code> for <code>TensorImageBase</code> instead.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">SpecNormalize</span><span class="p">(</span><span class="n">Normalize</span><span class="p">):</span>
    <span class="s2">"Normalize/denorm batch of `TensorImage`"</span>
    <span class="k">def</span> <span class="nf">encodes</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span><span class="n">TensorImageBase</span><span class="p">):</span> <span class="k">return</span> <span class="p">(</span><span class="n">x</span><span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">mean</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">std</span>
    <span class="k">def</span> <span class="nf">decodes</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span><span class="n">TensorImageBase</span><span class="p">):</span>
        <span class="n">f</span> <span class="o">=</span> <span class="n">to_cpu</span> <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span><span class="o">==</span><span class="s1">'cpu'</span> <span class="k">else</span> <span class="n">noop</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">x</span><span class="o">*</span><span class="n">f</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">std</span><span class="p">)</span> <span class="o">+</span> <span class="n">f</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">mean</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># make global and channel normalizers</span>
<span class="n">GlobalSpecNorm</span>  <span class="o">=</span> <span class="n">SpecNormalize</span><span class="p">(</span><span class="n">global_mean</span><span class="p">,</span>  <span class="n">global_std</span><span class="p">,</span>  <span class="n">axes</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
<span class="n">ChannelSpecNorm</span> <span class="o">=</span> <span class="n">SpecNormalize</span><span class="p">(</span><span class="n">channel_mean</span><span class="p">,</span> <span class="n">channel_std</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Training-with-different-statistics">
<a class="anchor" href="#Training-with-different-statistics" aria-hidden="true"><span class="octicon octicon-link"></span></a>Training with different statistics<a class="anchor-link" href="#Training-with-different-statistics"> </a>
</h1>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now for the moment of truth. We train with the two different spectrogram normalizations and measure their impact. For this we again follow the <code>fastaudio</code> baseline and train each type of normalization for 20 epochs. The final score is the averaged accuracy of five runs.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">epochs</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">num_runs</span> <span class="o">=</span> <span class="mi">5</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Finding-a-good-learning-rate.">
<a class="anchor" href="#Finding-a-good-learning-rate." aria-hidden="true"><span class="octicon octicon-link"></span></a>Finding a good learning rate.<a class="anchor-link" href="#Finding-a-good-learning-rate."> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The learning rate is arguably the most critical neural network hyperparameter. The <code>lr_find</code> function in <code>fastai</code> is a great empirical way to set a good learning rate. The default learning rate in <code>cnn_learner</code> (0.001) is a good starting point. But since our task is so different from natural images it is worth re-evaluating this assumption.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># auds = DataBlock(blocks=(AudioBlock, CategoryBlock),  </span>
<span class="c1">#                  get_x=ColReader("filename", pref=path/"audio"), </span>
<span class="c1">#                  splitter=CrossValidationSplitter(fold=1), </span>
<span class="c1">#                  item_tfms = [AudioNormalize], </span>
<span class="c1">#                  batch_tfms = [audio2spec, GlobalSpecNorm],</span>
<span class="c1">#                  get_y=ColReader("category"))</span>
<span class="c1"># dbunch = auds.dataloaders(df, bs=64)</span>

<span class="c1"># learn = cnn_learner(dbunch, </span>
<span class="c1">#                     xresnet18, </span>
<span class="c1">#                     pretrained=False,</span>
<span class="c1">#                     config=cnn_config(n_in=1),</span>
<span class="c1">#                     loss_fn=CrossEntropyLossFlat,</span>
<span class="c1">#                     metrics=[accuracy],)</span>
<span class="c1"># learn.lr_find()</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>It seems we can make the learning rate higher than the default. 2e-3 looks like a good point in the curve, and we could potentially go even higher.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">good_lr</span> <span class="o">=</span> <span class="mf">2e-3</span> <span class="c1"># from find_lr</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Training-helpers">
<a class="anchor" href="#Training-helpers" aria-hidden="true"><span class="octicon octicon-link"></span></a>Training helpers<a class="anchor-link" href="#Training-helpers"> </a>
</h2>
<p>To avoid repeating ourselves, the helper functions below will build dataloaders and run the training loops.<br>
The <code>get_dls</code> function makes it clear which normalization is being used. The <code>train_loops</code> function repeats training runs a given number of times.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">get_dls</span><span class="p">(</span><span class="n">bs</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">item_tfms</span><span class="o">=</span><span class="p">[],</span> <span class="n">batch_tfms</span><span class="o">=</span><span class="p">[]):</span>
    <span class="s2">"Get dataloaders with given `bs` and batch/item tfms."</span>
    <span class="n">auds</span> <span class="o">=</span> <span class="n">DataBlock</span><span class="p">(</span><span class="n">blocks</span><span class="o">=</span><span class="p">(</span><span class="n">AudioBlock</span><span class="p">,</span> <span class="n">CategoryBlock</span><span class="p">),</span>  
                     <span class="n">get_x</span><span class="o">=</span><span class="n">ColReader</span><span class="p">(</span><span class="s2">"filename"</span><span class="p">,</span> <span class="n">pref</span><span class="o">=</span><span class="n">path</span><span class="o">/</span><span class="s2">"audio"</span><span class="p">),</span> 
                     <span class="n">splitter</span><span class="o">=</span><span class="n">CrossValidationSplitter</span><span class="p">(</span><span class="n">fold</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
                     <span class="n">item_tfms</span><span class="o">=</span><span class="n">item_tfms</span><span class="p">,</span>   <span class="c1"># for waveform normalization</span>
                     <span class="n">batch_tfms</span><span class="o">=</span><span class="n">batch_tfms</span><span class="p">,</span> <span class="c1"># for spectrogram normalization</span>
                     <span class="n">get_y</span><span class="o">=</span><span class="n">ColReader</span><span class="p">(</span><span class="s2">"category"</span><span class="p">))</span>
    <span class="n">dls</span> <span class="o">=</span> <span class="n">auds</span><span class="o">.</span><span class="n">dataloaders</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">bs</span><span class="o">=</span><span class="n">bs</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">dls</span>

<span class="k">def</span> <span class="nf">train_loops</span><span class="p">(</span><span class="n">dls</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">num_runs</span><span class="o">=</span><span class="n">num_runs</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="n">epochs</span><span class="p">):</span>
    <span class="s2">"Runs `num_runs` training loops with `dls` for given `epochs`."</span>
    <span class="n">accuracies</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_runs</span><span class="p">):</span>
        <span class="n">learn</span> <span class="o">=</span> <span class="n">cnn_learner</span><span class="p">(</span><span class="n">dls</span><span class="p">,</span> 
                            <span class="n">xresnet18</span><span class="p">,</span> 
                            <span class="n">pretrained</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                            <span class="n">config</span><span class="o">=</span><span class="n">cnn_config</span><span class="p">(</span><span class="n">n_in</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
                            <span class="n">loss_fn</span><span class="o">=</span><span class="n">CrossEntropyLossFlat</span><span class="p">,</span>
                            <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="n">accuracy</span><span class="p">],)</span>
        <span class="n">learn</span><span class="o">.</span><span class="n">fit_one_cycle</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">good_lr</span><span class="p">)</span>
        <span class="n">accuracies</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">learn</span><span class="o">.</span><span class="n">recorder</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Average accuracy for "</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s1">": </span><span class="si">{</span><span class="nb">sum</span><span class="p">(</span><span class="n">accuracies</span><span class="p">)</span> <span class="o">/</span> <span class="n">num_runs</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Baseline-performance">
<a class="anchor" href="#Baseline-performance" aria-hidden="true"><span class="octicon octicon-link"></span></a>Baseline performance<a class="anchor-link" href="#Baseline-performance"> </a>
</h2>
<p>Before getting carried away with normalization, we have to first know where we stand. Setting a baseline without normalizations means we can later evaluate the impact of normalization. Else we cannot know if normalization helped at all.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># # data without normalization</span>
<span class="c1"># dls = get_dls(batch_tfms=[audio2spec])</span>
<span class="c1"># # run training loops</span>
<span class="c1"># train_loops(dls, name='No Norm')</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Performance-with-global-statistics">
<a class="anchor" href="#Performance-with-global-statistics" aria-hidden="true"><span class="octicon octicon-link"></span></a>Performance with global statistics<a class="anchor-link" href="#Performance-with-global-statistics"> </a>
</h2>
<p>Next we normalize each audio waveform and the spectrograms with scalar global statistics.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># # data with waveform and global normalization</span>
<span class="c1"># dls = get_dls(item_tfms=[AudioNormalize],</span>
<span class="c1">#               batch_tfms=[audio2spec, GlobalSpecNorm])</span>
<span class="c1"># # run training loops</span>
<span class="c1"># train_loops(dls, name='Global Norm')</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Performance-with-channel-statistics">
<a class="anchor" href="#Performance-with-channel-statistics" aria-hidden="true"><span class="octicon octicon-link"></span></a>Performance with channel statistics<a class="anchor-link" href="#Performance-with-channel-statistics"> </a>
</h2>
<p>Finally, we normalize each audio waveform and the spectrograms with channel-based statistics.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># # get data with waveform and channel normalization</span>
<span class="c1"># dls = get_dls(item_tfms=[AudioNormalize],</span>
<span class="c1">#               batch_tfms=[audio2spec, ChannelSpecNorm])</span>
<span class="c1"># # run training loops</span>
<span class="c1"># train_loops(dls, name='Channel Norm')</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Conclusions">
<a class="anchor" href="#Conclusions" aria-hidden="true"><span class="octicon octicon-link"></span></a>Conclusions<a class="anchor-link" href="#Conclusions"> </a>
</h1>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In this post we explored some issues around normalizing spectrograms for neural network training. We saw how spectrograms are fundamentally different than natural images and how there are at least two ways of normalizing them.</p>
<p>We then implemented these two normalization techniques and tested them against a baseline in the <code>fastaudio</code> ESC-50 competition.</p>
<p>There was a noticeable gain from using the global type of normalization, and a moderate gain from using the channel-based normalization. This makes intuitive sense, since a simple <code>show_batch</code> showed how varied these spectrograms were. In other words, there is a large amount of intra and inter channel variability both within and across classes. Under these conditions we'd expect that a more general, global normalization better suits this task. If the samples all came from a consistent source, say speech spectrograms, then the per-channel normalization might fare better.</p>
<p>However, at the end of the day, there is no single theoretically correct way to normalize spectrograms for deep neural networks. Like many aspects of this field, the design choices will be experimental and depend on both the domain and problem specifics.</p>
<p>I hope this post gave you a good notion for how to normalize spectrograms. I also hope it gave you some ideas, and potential approaches to try yourself! The ESC-50 challenge is an excellent playground to try them out.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="For-fun,-going-as-high-as-we-can-based-on-ImageNette">
<a class="anchor" href="#For-fun,-going-as-high-as-we-can-based-on-ImageNette" aria-hidden="true"><span class="octicon octicon-link"></span></a>For fun, going as high as we can based on ImageNette<a class="anchor-link" href="#For-fun,-going-as-high-as-we-can-based-on-ImageNette"> </a>
</h1>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># from fastai.basics import *</span>
<span class="c1"># from fastai.vision.all import *</span>
<span class="c1"># from fastai.callback.all import *</span>
<span class="c1"># from fastai.distributed import *</span>
<span class="c1"># from fastprogress import fastprogress</span>
<span class="c1"># from torchvision.models import *</span>
<span class="c1"># from fastai.vision.models.xresnet import *</span>
<span class="c1"># from fastai.callback.mixup import *</span>
<span class="c1"># from fastscript import *</span>

<span class="c1"># auds = DataBlock(blocks=(AudioBlock, CategoryBlock),  </span>
<span class="c1">#                  get_x=ColReader("filename", pref=path/"audio"), </span>
<span class="c1">#                  splitter=CrossValidationSplitter(fold=1),</span>
<span class="c1">#                  item_tfms = [AudioNormalize], </span>
<span class="c1">#                  batch_tfms = [audio2spec, GlobalSpecNorm],</span>
<span class="c1">#                  get_y=ColReader("category"))</span>
<span class="c1"># dbunch = auds.dataloaders(df, bs=64)</span>

<span class="c1"># # with mixup, train for longer</span>
<span class="c1"># epochs = 80</span>

<span class="c1"># # ranger optimizer </span>
<span class="c1"># lr      = 2e-3</span>
<span class="c1"># mom     = 0.9</span>
<span class="c1"># sqrmom  = 0.99</span>
<span class="c1"># eps     = 1e-6</span>
<span class="c1"># beta    = 0</span>
<span class="c1"># opt_func = partial(ranger, mom=mom, sqr_mom=sqrmom, eps=eps, beta=beta)</span>

<span class="c1"># # default pooling</span>
<span class="c1"># pool = AvgPool</span>

<span class="c1"># # mish activation</span>
<span class="c1"># act_fn = Mish</span>

<span class="c1"># # loss function</span>
<span class="c1"># loss_func = LabelSmoothingCrossEntropyFlat()</span>

<span class="c1"># # with mixup augmentation</span>
<span class="c1"># mixup = True</span>
<span class="c1"># alpha = 0.4</span>
<span class="c1"># cbs = MixUp(alpha) if mixup else []</span>

<span class="c1"># # whether to use self-attention</span>
<span class="c1"># sa  = False</span>
<span class="c1"># sym = False</span>

<span class="c1"># # weight decay</span>
<span class="c1"># wd = 1e-2</span>

<span class="c1"># # the context manager way of dp/ddp, both can handle single GPU base case.</span>
<span class="c1"># gpu = 0</span>
<span class="c1"># n_gpu = torch.cuda.device_count()</span>
<span class="c1"># # ctx = learn.parallel_ctx if gpu is None and n_gpu else learn.distrib_ctx</span>
<span class="c1"># # ctx = learn.distrib_ctx</span>

<span class="c1"># # # model</span>
<span class="c1"># # n_out = 50</span>
<span class="c1"># # m = xse_resnext18</span>
<span class="c1"># # model = m(n_out=n_out, c_in=1, act_cls=act_fn, sa=sa, sym=sym, pool=pool)</span>

<span class="c1"># accuracies = []</span>
<span class="c1"># for run in range(num_runs):</span>
<span class="c1">#     print(f'Run: {run}')</span>
<span class="c1"># #     learn = Learner(dbunch, , opt_func=opt_func, \</span>
<span class="c1"># #             metrics=[accuracy], loss_func=loss_func)</span>
<span class="c1">#     learn = cnn_learner(dbunch, </span>
<span class="c1">#                         xse_resnext18,</span>
<span class="c1">#                         pretrained=False,</span>
<span class="c1">#                         config=cnn_config(n_in=1),</span>
<span class="c1">#                         loss_fn=CrossEntropyLossFlat,</span>
<span class="c1">#                         opt_func=opt_func,</span>
<span class="c1">#                         metrics=[accuracy])</span>
<span class="c1">#     ctx = learn.parallel_ctx if gpu is None and n_gpu else learn.distrib_ctx</span>
<span class="c1">#     with partial(ctx, gpu)(): # distributed traing requires "-m fastai.launch"</span>
<span class="c1">#         print(f"Training in {ctx.__name__} context on GPU {gpu if gpu is not None else list(range(n_gpu))}")</span>
<span class="c1">#         learn.fit_flat_cos(epochs, lr, wd=wd, cbs=cbs)</span>
<span class="c1"># #         learn.fine_tune(epochs, lr, wd=wd, cbs=cbs)</span>
<span class="c1">#     accuracies.append(learn.recorder.values[-1][-1])</span>

<span class="c1"># print(f'Average accuracy for ImageNet training: {sum(accuracies) / num_runs}')</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">

</div>
</div>
</div>
</div>



  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="enzokro/clck10"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/normalizing_spectrums/2020/08/04/Normalizing-spectrograms-for-deep-learning.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>DSP, RF, and ML posts</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/enzokro" title="enzokro"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/clck10" title="clck10"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
