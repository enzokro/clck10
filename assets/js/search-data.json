{
  
    
        "post0": {
            "title": "How to normalize spectrograms",
            "content": "Introduction . Spectrograms are often treated as images in deep learning to leverage the many great techniques from image classification. A spectrogram, however, is fundamentally different from natural images in several ways as we will see below. That raises the central question of this post: what is the proper way to normalize spectrograms when training neural networks? . Turning audio into an image . It is hard to overstate the success of deep neural networks for classifying images. This challenging task was previously dominated by expert handcrafted features. Now, the features are automatically learned from labeled data instead. The performance of these learned features completely shifted the Computer Vision paradigm. We would ideally like to follow these same, proven approaches in audio tasks. . However, audio is usually treated as a one dimensional signal in Machine Learning applications. Even stereo recordings with more than one channel are first mixed into mono (single channel) before processing. That means raw audio is unusable with 2D CNNs which are the bread and butter of modern image recognition. If we could represent audio in two dimensions, like an image, it opens up a host of deep learning image classification techniques. . Thankfully there are many ways to transform audio into two dimensions. The most common one is the short-time Fourier Transform (STFT). The STFT turns audio into a spectrogram: a 2-D signal representation in time and frequency. Since a spectrogram is two dimensional we can treat it like an image! . Before plugging spectrograms into a neural network we need a data pipeline. Data normalization is one of the first steps in any good data pipeline. This is because learning is difficult with unnormalized inputs and convergence could take a very long time. For natural images, normalization involves: . Centering the image values by subtracting a single, scalar mean. | Dividing the images by a scalar standard deviation to give them a variance of 1. | . Sample audio dataset . To keep things practical, we apply these normalization techniques to a sound classification challenge hosted by fastaudio. fastaudio is a community extension of the fastai library to make neural network audio tasks more accessible. The challenge here is to classify sounds in the ESC-50 dataset. ESC-50 stands for &quot;Environment Sound Classification with 50 classes&quot;. This set has a diverse set of sounds which gives a feel for how different audio spectrograms can be. Every file is five seconds long which makes batch processing easier since the samples are of the same size. . Many lines below are based on the fastaudio baseline results notebook. . Importing fastai and fastaudio modules . from fastai.vision.all import * from fastaudio.core.all import * from fastaudio.augment.all import * . Downloading the ESC-50 dataset . The first step is downloading the ESC-50 dataset. It is included in fastaudio so we grab it with fastai&#39;s untar_data. . # already in fastaudio, can download with fastai&#39;s `untar_data` path = untar_data(URLs.ESC50) . Looking inside of the downloaded audio folder reveals many .wav files. Below we view the files with the ls method, a handy fastai addition to python&#39;s standard pathlib.Path class. . wavs = (path/&quot;audio&quot;).ls() wavs . The output of ls shows there are 2,000 audio files. But the filenames are not too descriptive, so how can we know what is actually in each one? As with many datasets, the download includes a table with more information about the data (aka metadata). . # read the audio metadata and show its first few rows df = pd.read_csv(path/&quot;meta&quot;/&quot;esc50.csv&quot;) df.head() . The key information from the table is in the filename and category columns. The filename gives the name of the file inside of the audio folder. The category tells us which class it belongs to. . We pick the last file in the data directory as our working example. The file&#39;s name can index into the metadata table above to display its specific information. . # pick the row where &quot;filename&quot; matches the waveform&#39;s &quot;name&quot;. df.loc[df.filename == wavs[-1].name] . This is a recording of crickets! We can load this audio file using the create function of the AudioTensor class in fastaudio. AudioTensor wraps a torch.Tensor with some extra syntactic sugar. . # create an AudioTensor from a file path sample = AudioTensor.create(wavs[-1]) . Thanks to AudioTensor we can directly plot and even listen to our sample. Each &quot;burst&quot; in the file is a cricket chirp. There are three full chirps and the early starts of a fourth chirp. . sample.show() . Normalizing waveforms . The first step is normalizing the audio waveform itself. We give it a mean of zero and unit variance in the typical way: . $$ text{norm_audio} = frac{ text{audio} - mean( text{audio})}{std( text{audio})} $$ . # normalize the waveform norm_sample = (sample - sample.mean()) / sample.std() . Let&#39;s check if the mean is roughly 0 and the variance is roughly one: . # checking if normalization worked norm_sample.mean(),norm_sample.var() . Success! The waveform is normalized. . Extracting spectrograms from audio . We are now ready to extract a spectrogram from the normalized audio. The fastaudio library wraps parts of torchaudio to convert an AudioTensor into a spectrogram. . # create a fastaudio Transform that converts audio into spectrograms cfg = AudioConfig.BasicSpectrogram() audio2spec = AudioToSpec.from_cfg(cfg) . The spectrogram computation details are not important here. But a quick look at the spectrogram source code shows that it boils down to some pre and post processing around a torch.stft call. We can now transform our audio into a spectrogram. . # extract the spectrogram spec = audio2spec(norm_sample) . Just like with the waveform, fastaudio can directly plot spectrograms. The colorbar on the right is especially helpful here, since matplotlib always normalizes the values in a plot to a certain color range. Withtout the colorbar, the fixed color range makes it impossible to know or even guess the exact values in a spectrogram plot. . spec.show(); . This is a good time to compare the shapes of the audio vs. the spectrogram to see the added dimension that makes the spectrogram an &quot;image&quot;. . print(f&#39;Audio shape [channels, samples]: {norm_sample.shape}&#39;) print(f&#39;Spectrum shape [channels, bins, time_steps]: {spec.shape}&#39;) . How do we normalize spectrograms? . As stated in the introduction, a spectrogram is fundamentally different from an image. . Both dimensions in an image are in the spatial domain and have the same units. Images are stored as integers in the range of [0, 255]. To normalize first divide all pixels by 255, the max possible value, to map them into [0, 1]. Then, find the statistics that approximately center the data and give it unit variance. For a color image we have three channels (RGB) and normalize each one. If the image is grayscale then we normalize its single channel instead. Given the layout of natural images, and the fact that both dimensions are in the same domain, it makes sense to normalize each channel with single, global values. . Spectrograms are different. In a spectrogram, one dimension represents time and the other represents frequency. Different quantities, scales, and sizes. Frequency dimension given by choice of FFT size. Sets our spectral resolution. Time dimension given by length of our signal, FFT size, and window overlap. Sets our temporal resolution. In fact what we call the spectrogram is actually the log of the power spectrum. Below we give a quick recap of how the spectrogram is computed to show how it differs from images. . If $ text{x}$ is the input audio then the STFT returns the spectrum: $$ text{spectrum} = text{STFT(x)}$$ We are more interested in the energy or power of the signal, so we take the absolute value of the STFT and square it: $$ text{power_spectrum} = | text{STFT(x)}|^2$$ While we could use the power spectrum as our input &quot;image&quot;, it is a bit problematic. The power spectrum often has a few, strong peaks and many small values. This means the values are heavy-tailed and make for poor network inputs. To deal with this we take the log of the power spectrum which spreads out the values. This becomes the spectrogram: $$ text{spectrogram} = log(| text{STFT(x)}|^2)$$ The range of the log function is from $- infty$ to $+ infty$ which is very different than the integers from 0 to 255. . However, the spectrogram also introduces the notion of a different type of channel. This makes &quot;channel&quot; an overloaded term for our purposes but it is still a crucial piece of the puzzle. The spectrogram transform can be interpreted as a &quot;channelizer&quot;. That is a fancy way to say that it takes the continuous frequency spectrum of our signal and chops it up into discrete bins, or channels. For example, consider a signal sampled at 16 kHz (typical for audio) where we take an STFT of size 512. Our spectrogram will have 512 channels where each one has a &quot;bandwidth&quot; of $$16 text{kHz} / 512 text{bins} = 31.25 text{Hz per bin}$$ . Even though these spectrum channels are different from the channels in an image, it raises the question: should we (or can we?) normalize an entire spectrum &quot;image&quot; with a single, global value? Or do we need to normalize each channel, as is done with images? . There is no clear answer here, and your approach will likely depend both on the specifics of your problem and where your system will be deployed. For example, if your are building a system that will be deployed in a similar environment as the training one, then it might make more sense to normalize by channels. Your channel-based normalization statistics will follow the average noise floor and activity of the training data. This motivation hold if you expect roughly the same patterns and distributions of activity once the system is deployed. However, it will be critical to monitor the deployed environment and update the statistics as needed, else you slowly shift out of domain. . If your system will instead be used in a completely different environment, of which you have no knowledge, then the global statistics could be a better fit. While not as technically sound, your model won&#39;t be as surprised by radically new activity across different channels. . Lastly, we also have issue of Transfer Learning. In Transfer Learning it is best-practice to normalize the new dataset with the statistics from the old dataset. In most cases that means normalizing with ImageNet statistics. So if you are doing transfer learning, the easiest approach will be to use original stats. If your dataset is large enough that you are training from scratch, then the above applies. . Spectrogram Normalization . Normalization statistics always come from the training set (this is a crucial place to avoid data leakage from the validation set). That means stepping through the training set once and getting a mean and standard deviation from each mini-batch. Then, we average the statistics from each mini-batch to get a pair of &quot;global&quot; statistics. One small detail: if your training dataset is large enough, you often do not need to iterate through the entire set. It is enough to sample 10% to 20% of the dataset for accurate statistics. However, since ESC-50 is small enough we get statistics from the whole set. . First we to accumulate these statistics over mini-batches. We can borrow and slightly refactor a class from the incredibly helpful guide here. The StatsRecorder class below tracks the mean and standard deviation across mini-batches. . class StatsRecorder: def __init__(self, red_dims=(0,2,3)): &quot;&quot;&quot;Accumulates normalization statistics across mini-batches. ref: http://notmatthancock.github.io/2017/03/23/simple-batch-stat-updates.html &quot;&quot;&quot; self.red_dims = red_dims # which mini-batch dimensions to average over self.nobservations = 0 # running number of observations def update(self, data): &quot;&quot;&quot; data: ndarray, shape (nobservations, ndimensions) &quot;&quot;&quot; # initialize stats and dimensions on first batch if self.nobservations == 0: self.mean = data.mean(dim=self.red_dims, keepdim=True) self.std = data.std (dim=self.red_dims,keepdim=True) self.nobservations = data.shape[0] self.ndimensions = data.shape[1] else: if data.shape[1] != self.ndimensions: raise ValueError(&#39;Data dims do not match previous observations.&#39;) # find mean of new mini batch newmean = data.mean(dim=self.red_dims, keepdim=True) newstd = data.std(dim=self.red_dims, keepdim=True) # update number of observations m = self.nobservations * 1.0 n = data.shape[0] # update running statistics tmp = self.mean self.mean = m/(m+n)*tmp + n/(m+n)*newmean self.std = m/(m+n)*self.std**2 + n/(m+n)*newstd**2 + m*n/(m+n)**2 * (tmp - newmean)**2 self.std = torch.sqrt(self.std) # update total number of seen samples self.nobservations += n . By default StatsRecorder averages over the image channel dimensions (grayscale or RGB). The red_dims argument might look familiar from normalization code in other Computer Vision tasks (even the Normalize in fastai). Averaging instead over spectrogram channels is as easy as passing a different red_dims. . Building the dataset loader . Next we need to step through the training dataset. The setup belows follows the fastaudio ESC-50 baseline. One thing to mention is that by default, fastaudio resamples audio to 16 kHz. While much of the audio in the wild is sampled at 16 kHz, ESC-50 is actually sampled at a higher 44.1 kHz rate. Downsampling risks throwing away some information. But, keeping the higher sampling rate almost triples the &quot;width&quot; of the spectrogram. This much larger image potentially limits our batch size and architecture choices. For now we stick with downsampling to 16 kHz since it yields a very reasonable spectrogram shape of [201, 401]. . We also need a Transform that normalizes individual audio waveforms. . class AudioNormalize(Transform): &quot;Normalizes a single `AudioTensor`.&quot; def encodes(self, x:AudioTensor): return (x-x.mean()) / x.std() . def CrossValidationSplitter(col=&#39;fold&#39;, fold=1): &quot;Split `items` (supposed to be a dataframe) by fold in `col`&quot; def _inner(o): assert isinstance(o, pd.DataFrame), &quot;ColSplitter only works when your items are a pandas DataFrame&quot; col_values = o.iloc[:,col] if isinstance(col, int) else o[col] valid_idx = (col_values == fold).values.astype(&#39;bool&#39;) return IndexSplitter(mask2idxs(valid_idx))(o) return _inner # do not resample audio auds = DataBlock(blocks=(AudioBlock, CategoryBlock), get_x=ColReader(&quot;filename&quot;, pref=path/&quot;audio&quot;), splitter=CrossValidationSplitter(fold=1), item_tfms = [AudioNormalize], batch_tfms = [audio2spec], get_y=ColReader(&quot;category&quot;)) dbunch = auds.dataloaders(df, bs=64) dbunch.show_batch(figsize=(7,7)) . Finding spectrogram normalization statistics . Below we create two statistic recorders: one for global statistics and one for channel-based statistics. Then we step through the entire dataset and find the normalization stats. . # create recorders global_stats = StatsRecorder() channel_stats = StatsRecorder(red_dims=(0,1,3)) # step through the dataset with torch.no_grad(): for idx,(x,y) in enumerate(iter(dbunch.train)): # update normalization statistics global_stats.update(x) channel_stats.update(x) # parse out the stats global_mean,global_std = global_stats.mean,global_stats.std channel_mean,channel_std = channel_stats.mean,channel_stats.std . We can check the shape of both statistics to make sure they are as expected. For the global &quot;grayscale&quot; statistics, we expect a shape of: [1,1,1,1]. With spectrogram channel normalizations, we expect a shape of [1,1,201,1] with one normalization stat for each spectrogram bin. . print(&#39;Shapes of global mean/std:&#39;) global_mean.shape,global_std.shape . print(&#39;Shapes of channel mean/std:&#39;) channel_mean.shape,channel_mean.shape . Making Normalization Transforms . We need to extend the Normalize in fastai to normalize the spectrogram mini-batches. The reason is type dispatch. fastai normalization uses ImageNet statistics due to the focus on transfer learning with color images. But this ImageNet normalization is only defined for the TensorImage class, while AudioSpectrogram subclasses the different TensorImageBase. The solution is to define encodes and decodes for TensorImageBase instead. . class SpecNormalize(Normalize): &quot;Normalize/denorm batch of `TensorImage`&quot; def encodes(self, x:TensorImageBase): return (x-self.mean) / self.std def decodes(self, x:TensorImageBase): f = to_cpu if x.device.type==&#39;cpu&#39; else noop return (x*f(self.std) + f(self.mean)) . # make global and channel normalizers GlobalSpecNorm = SpecNormalize(global_mean, global_std, axes=(0,2,3)) ChannelSpecNorm = SpecNormalize(channel_mean, channel_std, axes=(0,1,3)) . Training with different statistics . Now for the moment of truth. We train with the two different spectrogram normalizations and measure their impact. For this we again follow the fastaudio baseline and train each type of normalization for 20 epochs. The final score is the averaged accuracy of five runs. . epochs = 20 num_runs = 5 . Finding a good learning rate. . The learning rate is arguably the most critical neural network hyperparameter. The lr_find function in fastai is a great empirical way to set a good learning rate. The default learning rate in cnn_learner (0.001) is a good starting point. But since our task is so different from natural images it is worth re-evaluating this assumption. . # auds = DataBlock(blocks=(AudioBlock, CategoryBlock), # get_x=ColReader(&quot;filename&quot;, pref=path/&quot;audio&quot;), # splitter=CrossValidationSplitter(fold=1), # item_tfms = [AudioNormalize], # batch_tfms = [audio2spec, GlobalSpecNorm], # get_y=ColReader(&quot;category&quot;)) # dbunch = auds.dataloaders(df, bs=64) # learn = cnn_learner(dbunch, # xresnet18, # pretrained=False, # config=cnn_config(n_in=1), # loss_fn=CrossEntropyLossFlat, # metrics=[accuracy],) # learn.lr_find() . It seems we can make the learning rate higher than the default. 2e-3 looks like a good point in the curve, and we could potentially go even higher. . good_lr = 2e-3 # from find_lr . Training helpers . To avoid repeating ourselves, the helper functions below will build dataloaders and run the training loops. The get_dls function makes it clear which normalization is being used. The train_loops function repeats training runs a given number of times. . def get_dls(bs=64, item_tfms=[], batch_tfms=[]): &quot;Get dataloaders with given `bs` and batch/item tfms.&quot; auds = DataBlock(blocks=(AudioBlock, CategoryBlock), get_x=ColReader(&quot;filename&quot;, pref=path/&quot;audio&quot;), splitter=CrossValidationSplitter(fold=1), item_tfms=item_tfms, # for waveform normalization batch_tfms=batch_tfms, # for spectrogram normalization get_y=ColReader(&quot;category&quot;)) dls = auds.dataloaders(df, bs=bs) return dls def train_loops(dls, name, num_runs=num_runs, epochs=epochs): &quot;Runs `num_runs` training loops with `dls` for given `epochs`.&quot; accuracies = [] for i in range(num_runs): learn = cnn_learner(dls, xresnet18, pretrained=False, config=cnn_config(n_in=1), loss_fn=CrossEntropyLossFlat, metrics=[accuracy],) learn.fit_one_cycle(epochs, good_lr) accuracies.append(learn.recorder.values[-1][-1]) print(f&#39;Average accuracy for &quot;{name}&quot;: {sum(accuracies) / num_runs}&#39;) . Baseline performance . Before getting carried away with normalization, we have to first know where we stand. Setting a baseline without normalizations means we can later evaluate the impact of normalization. Else we cannot know if normalization helped at all. . # # data without normalization # dls = get_dls(batch_tfms=[audio2spec]) # # run training loops # train_loops(dls, name=&#39;No Norm&#39;) . Performance with global statistics . Next we normalize each audio waveform and the spectrograms with scalar global statistics. . # # data with waveform and global normalization # dls = get_dls(item_tfms=[AudioNormalize], # batch_tfms=[audio2spec, GlobalSpecNorm]) # # run training loops # train_loops(dls, name=&#39;Global Norm&#39;) . Performance with channel statistics . Finally, we normalize each audio waveform and the spectrograms with channel-based statistics. . # # get data with waveform and channel normalization # dls = get_dls(item_tfms=[AudioNormalize], # batch_tfms=[audio2spec, ChannelSpecNorm]) # # run training loops # train_loops(dls, name=&#39;Channel Norm&#39;) . Conclusions . In this post we explored some issues around normalizing spectrograms for neural network training. We saw how spectrograms are fundamentally different than natural images and how there are at least two ways of normalizing them. . We then implemented these two normalization techniques and tested them against a baseline in the fastaudio ESC-50 competition. . There was a noticeable gain from using the global type of normalization, and a moderate gain from using the channel-based normalization. This makes intuitive sense, since a simple show_batch showed how varied these spectrograms were. In other words, there is a large amount of intra and inter channel variability both within and across classes. Under these conditions we&#39;d expect that a more general, global normalization better suits this task. If the samples all came from a consistent source, say speech spectrograms, then the per-channel normalization might fare better. . However, at the end of the day, there is no single theoretically correct way to normalize spectrograms for deep neural networks. Like many aspects of this field, the design choices will be experimental and depend on both the domain and problem specifics. . I hope this post gave you a good notion for how to normalize spectrograms. I also hope it gave you some ideas, and potential approaches to try yourself! The ESC-50 challenge is an excellent playground to try them out. . For fun, going as high as we can based on ImageNette . # from fastai.basics import * # from fastai.vision.all import * # from fastai.callback.all import * # from fastai.distributed import * # from fastprogress import fastprogress # from torchvision.models import * # from fastai.vision.models.xresnet import * # from fastai.callback.mixup import * # from fastscript import * # auds = DataBlock(blocks=(AudioBlock, CategoryBlock), # get_x=ColReader(&quot;filename&quot;, pref=path/&quot;audio&quot;), # splitter=CrossValidationSplitter(fold=1), # item_tfms = [AudioNormalize], # batch_tfms = [audio2spec, GlobalSpecNorm], # get_y=ColReader(&quot;category&quot;)) # dbunch = auds.dataloaders(df, bs=64) # # with mixup, train for longer # epochs = 80 # # ranger optimizer # lr = 2e-3 # mom = 0.9 # sqrmom = 0.99 # eps = 1e-6 # beta = 0 # opt_func = partial(ranger, mom=mom, sqr_mom=sqrmom, eps=eps, beta=beta) # # default pooling # pool = AvgPool # # mish activation # act_fn = Mish # # loss function # loss_func = LabelSmoothingCrossEntropyFlat() # # with mixup augmentation # mixup = True # alpha = 0.4 # cbs = MixUp(alpha) if mixup else [] # # whether to use self-attention # sa = False # sym = False # # weight decay # wd = 1e-2 # # the context manager way of dp/ddp, both can handle single GPU base case. # gpu = 0 # n_gpu = torch.cuda.device_count() # # ctx = learn.parallel_ctx if gpu is None and n_gpu else learn.distrib_ctx # # ctx = learn.distrib_ctx # # # model # # n_out = 50 # # m = xse_resnext18 # # model = m(n_out=n_out, c_in=1, act_cls=act_fn, sa=sa, sym=sym, pool=pool) # accuracies = [] # for run in range(num_runs): # print(f&#39;Run: {run}&#39;) # # learn = Learner(dbunch, , opt_func=opt_func, # # metrics=[accuracy], loss_func=loss_func) # learn = cnn_learner(dbunch, # xse_resnext18, # pretrained=False, # config=cnn_config(n_in=1), # loss_fn=CrossEntropyLossFlat, # opt_func=opt_func, # metrics=[accuracy]) # ctx = learn.parallel_ctx if gpu is None and n_gpu else learn.distrib_ctx # with partial(ctx, gpu)(): # distributed traing requires &quot;-m fastai.launch&quot; # print(f&quot;Training in {ctx.__name__} context on GPU {gpu if gpu is not None else list(range(n_gpu))}&quot;) # learn.fit_flat_cos(epochs, lr, wd=wd, cbs=cbs) # # learn.fine_tune(epochs, lr, wd=wd, cbs=cbs) # accuracies.append(learn.recorder.values[-1][-1]) # print(f&#39;Average accuracy for ImageNet training: {sum(accuracies) / num_runs}&#39;) .",
            "url": "https://enzokro.dev/normalizing_spectrums/2020/08/04/Normalizing-spectrograms-for-deep-learning.html",
            "relUrl": "/normalizing_spectrums/2020/08/04/Normalizing-spectrograms-for-deep-learning.html",
            "date": " • Aug 4, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Rayleigh Weight Initializations",
            "content": "Overview . This is the first in a series of posts that will build complex-valued neural networks in pytorch with an eye towards Radio Frequency (RF) applications. Part one introduces complex weight initializations using two pieces of information: . Magnitude: the length of a vector | Phase: where a vector is pointing | . The magnitudes will be drawn from a Rayleigh distribution which is a natural representation for RF data. Next, we make sure the weights are good initial values for neural networks by having their variance meet certain criteria. We then put everything together by creating complex weights as pytorch tensors. To start, we give some backgroud on weight initializations and why they are important. . Background on initializations . It is crucial to train neural networks from properly initialized weights. While initializations are now taken for granted, they were one of the key pieces that made it possible to train deep networks in the first place. The main insights about good weight initializations came from analyzing the variance of weights. Specifically, in how the weight&#39;s variance affected gradients during backpropagation. It was great work by He and Glorot, Bengio that showed how the variance of the weights should meet certain criteria to keep gradients flowing smoothly. Here, &quot;smoothly&quot; means that during training the gradients neither disappear (go to 0) nor explode (grow without bound). The best known initializations are now the defaults in popular deep learning libraries such as TensorFlow and pytorch. However, complex-valued deep networks are newer and not as well established. . Complex networks . Complex-valued networks have a long history, see Chapter 3 of this thesis for a great recap. The first modern and complete take on deep complex neural nets is likely Deep Complex Networks by Trabelsi et. al. This paper explored many fundamental building blocks for deep complex networks. It developed initializations, convolutions, activations, batch normalizations, and put them together into complex Residual Networks (ResNets). Despite this fantastic work the field stayed quiet at first. But, there has been a recent spike in activity with follow ups in: medical imaging, radio frequency (RF) signal processing, physical optical networks, and even some quantum networks! Next we take the first steps in making complex weights: creating their magnitudes. . Rayleigh Distribution . To best describe a Rayleigh distribution, imagine setting up a sensor that measures wind speed out in an open field. If we analyze the wind speed passing through this sensor in two directions, say North and East, then the magnitude of the wind&#39;s velocity will be Rayleigh distributed. A Rayleigh distribution happens in general when two random variables are added. To be Rayleigh distributed, the random variables must be uncorrelated, normally distributed, zero mean, and share the same standard deviation. A more relevant example for our purposes is tuning in to a radio signal. Imagine we tune a radio to an empty spectrum region where all we hear is noise. If we record the real and imaginary components of this RF stream, then their magnitudes will be Rayleigh distributed. In other words, the magnitude of RF noise follows a Rayleigh distribution. . Let&#39;s dive into the details. The equation below is the Probability Density Function (PDF) of a Rayleigh distribution. . $$f(x, sigma) = frac{x}{ sigma^2}e^{-x^2/(2 sigma^2)}, x geq 0$$ . If this equation looks intimidating, we can instead code it up as a python function to make it much clearer: . def rayleigh_pdf(x, sigma): &quot;Finds Rayleigh PDF evaluated at `x`.&quot; p = (x / sigma**2) * np.exp(-x**2 / (2*sigma**2)) return p . In the equation and code above sigma ($ sigma$) is the scale parameter. It is common in many probability distributions and usually controls how spread out or narrow a distribution is. . Let&#39;s start by setting $ sigma = 1$ to see the &quot;basic&quot; Rayleigh shape. We will then change sigma to see how it affects the distribution. . sigma = 1 # calculate PDF on 100 equally spaced points between 0 and 5 points = np.linspace(0, 5, 100) ray_pdf = rayleigh_pdf(points, sigma) . # plot the rayleigh pdf fig,ax = plt.subplots(figsize=(8,7)) ax.plot(ray_pdf) ax.set_xticklabels([-1] + list(range(len(ax.get_xticklabels())))) ax.set_xlabel(&#39;Sample Value&#39;, fontsize=&#39;x-large&#39;) ax.set_ylabel(&#39;Probability Density&#39;, fontsize=&#39;x-large&#39;) ax.set_title(&#39;Rayleigh PDF&#39;, fontsize=&#39;xx-large&#39;); . As we mentioned the scale, $ sigma$, changes the width or narrowness of the distribution. Let&#39;s both halve and double sigma to see what happens. . # setup plot fig,ax = plt.subplots(figsize=(8,7)) ax.set_xlabel(&#39;Sample Value&#39;, fontsize=&#39;x-large&#39;) ax.set_ylabel(&#39;Probability Density&#39;, fontsize=&#39;x-large&#39;) ax.set_title(&#39;Rayleigh PDFs&#39;, fontsize=&#39;xx-large&#39;); # plot sigmas in different colors sigmas = [0.5, 1, 2] colors = [&#39;m&#39;, &#39;b&#39;, &#39;r&#39;] for color,sig in zip(colors,sigmas): rpdf = rayleigh_pdf(points, sig) ax.plot(points, rpdf, c=color, label=f&#39;σ: {sig}&#39;) ax.set_xticklabels([-1] + list(range(len(ax.get_xticklabels())))) ax.legend(); . The blue line in plot above is the same PDF from the first plot where $ sigma = 1$. We can see how $ sigma = 0.5$ pulls the distribution up and to the left, while $ sigma = 2$ squishes it down and to the right. In other words, a smaller sigma makes our distribution narrower while a larger sigma makes it wider. . Plotting the theoretical Rayleigh PDF above only shows what the distribution should looks like. Now we need to actually generate the Rayleigh values. . Generating Rayleigh samples . We will use the RandomState class in the numpy library to generate Rayleigh samples. RandomState is a helpful class that can sample from just about every known distribution. . from numpy.random import RandomState . First we create the RandomState class with the chosen seed of $0$. . seed = 0 rand = RandomState(seed) . This class can directly sample from a Rayleigh distribution. We use the sampling function RandomState.rayleigh which accepts two parameters: . scale: $ sigma$ with a default value of 1. | size: the shape of the output array | . Let&#39;s start by drawing 1,000 Rayleigh samples with $ sigma = 1$. . sigma = 1 shape = 1000 # one dimensional vector with 1000 samples . ray_vals = rand.rayleigh(scale=sigma, size=shape) . How to check if these samples are actually Rayleigh distributed? We can refer back to our PDF plots at the beginning, which tell us how Rayleigh samples should be &quot;spread out&quot;. The easiest way to check if these samples are spread out as expected is with a histogram. . # plot histogram of 1000 drawn samples plt.figure(figsize=(8,7)) plt.hist(ray_vals) plt.xlabel(&#39;Sample Value&#39;, fontsize=&#39;x-large&#39;) plt.ylabel(&#39;Counts&#39;, fontsize=&#39;x-large&#39;) plt.title(f&#39;Histogram of {shape:,} Rayleigh samples&#39;, fontsize=&#39;xx-large&#39;); . That looks pretty good! As we generate more samples it should get even closer to the earlier PDF plots. Let&#39;s make sure this happens by now drawing 10,000 samples and using more bins. . large_shape = 10000 many_ray_vals = rand.rayleigh(scale=sigma, size=large_shape) plt.figure(figsize=(8,7)) plt.hist(many_ray_vals, bins=20); plt.xlabel(&#39;Sample Value&#39;, fontsize=&#39;x-large&#39;) plt.ylabel(&#39;Counts&#39;, fontsize=&#39;x-large&#39;) plt.title(f&#39;Histogram of {large_shape:,} Rayleigh samples&#39;, fontsize=&#39;xx-large&#39;); . Looking better. Let&#39;s compare this histogram against the theoretical Rayleigh PDF. Note that we pass density=True to the histogram function below to make it an approximate PDF. . # compare theoretical vs. sampled Rayleigh PDFs plt.figure(figsize=(8,7)) plt.hist(many_ray_vals, density=True, bins=20) # makes the histogram sum to one, to mimic pdf plt.plot(points, ray_pdf, c=&#39;r&#39;, label=&#39;theoretical rayleigh pdf&#39;, linewidth=3) plt.title(&#39;Sampled vs. Theoretical Rayleigh PDFs&#39;, fontsize=&#39;x-large&#39;) plt.xlabel(&#39;Sample Value&#39;, fontsize=&#39;x-large&#39;) plt.ylabel(&#39;Probability Density&#39;, fontsize=&#39;x-large&#39;) plt.legend(); . Adding phase information . With the Rayleigh values from above we now have the magnitude, or length, of the complex weights. But that is only one part of a complex number. We are still missing the phase, or angle, information. The phase tells us which direction in the complex plane a vector is pointing in. . For our purposes it is enough to use random angles. Why? Many processes such as speech, images, and RF modulations encode information in phase. We do not want to bias the networks to any of these particular phase setups. By adding random uniform phase, it is like starting with many vectors pointing out in all directions. Then, during training, the network learns how to best orient and scale the weights for its task. . Adding this random uniform phase is straightforward. We pick uniform samples from $- pi$ to $ pi$ radians which maps to a full loop of the unit circle. This can be done with the same RandomState class from before. . # pick random directions along the unit circle phase = rand.uniform(low=-np.pi, high=np.pi, size=ray_vals.shape) . The phase can split the Rayleigh magnitudes into real and imaginary components. We use the cosine of the phase for the real part, and the sine of the phase for the imaginary part. . real = ray_vals * np.cos(phase) imag = ray_vals * np.sin(phase) . Let&#39;s plot some of these values in the complex plane to see if we truly have randomly oriented vectors. We pick 100 random weights and expect to see vectors with different magnitudes spread out in all directions. . # plot a few complex weights chosen_samples = range(100) # plot 100 random samples plt.figure(figsize=(8,7)) for idx in chosen_samples: # index into phase and magnitude variables angle,mag = phase[idx],ray_vals[idx] plt.polar([0,angle],[0,mag], marker=&#39;o&#39;) # plot them starting from the origin . Matching He and Glorot variance criteria . Even though we have real and imaginary components, they are not good initializations yet. The polar plot above gives some clues as to why (hint: look at the range of magnitudes in the vectors). Recall from the background on initializations: the key insight was that the variance of the weights should meet certain criteria. Matching this criteria helps the gradients flow well during backpropagation. . To get more specific, both the He and Glorot criteria are based on the incoming and outgoing connections in a network layer. The number of connections are typically called fan_in and fan_out, respectively. . The He criteria says that the variance of weights $W$ should be: $$ text{Var}(W) = frac{2}{ text{fan_in}}$$ . The Glorot criteria says that the variance should be: $$ text{Var}(W) = frac{2}{ text{fan_in + fan_out}}$$ . Deep networks typically have hundreds or thousands of connections. In practice this means that the variance of the weights has to be very small. Now we can see why the values in the earlier polar plot are not good: their variance is clearly too large. . How can we generate Rayleigh samples that meet the He or Glorot variance criteria? The complex nets paper from earlier includes a nice derivation for the variance of a complex Rayleigh distribution: $$ text{Var}(W) = 2 sigma^{2}$$ We can set the Rayleigh variance equal to the He and Glorot criteria and solve for sigma. . To meet the He criteria, sigma should be: $$ sigma_{ text{He}} = frac{1}{ sqrt{ text{fan_in}}}$$ . To meet the Glorot criteria, sigma should be: $$ sigma_{ text{Glorot}} = frac{1}{ sqrt{ text{fan_in + fan_out}}}$$ . Let&#39;s take a step back. In the earlier sections we used a flat vector of complex weights as an example. It&#39;s as if we took a single series of wind velocity or RF sample measurements. Since the He and Glorot criteria are defined specifically for network layers, we now switch to a simple one-layer network as an example. We aribtrarily choose a layer with 100 inputs and 50 outputs (fan_in = 100, fan_out = 50). . Plugging in these fan_in and fan_out numbers into the sigma equations gives: $$ sigma_{ text{He}} = frac{1}{10}$$ . $$ sigma_{ text{Glorot}} = frac{1}{5 sqrt{6}}$$ . By plugging in these sigmas into the random sampler, it will draw Rayleigh values that match the chosen variance criteria. We can then add phase information in the same way as before. . Putting it all together: complex pytorch initializer . Here is a short recap of the previous sections: . Drew a flat series of Rayleigh magnitudes. | Picked a random phase component, then split the magnitudes into real and imaginary parts. | Saw how to match the He and Glorot criteria with Rayleigh samples for a single network layer. | To make the above practical and usable, we need to automatically generate complex weigths that: . Match the He/Glorot variance criteria | Are pytorch tensors | Have the correct shape for the given network layer | . We can do this in a python function. . One quick word about calculating fan_in and fan_out values. We saw the feed-forward case with our single network layer. There the number of incoming connections was simply fan_in and the outgoing connections were fan_out. However, the convolutional case is slightly more complicated. A convolutional layer has input and output feature maps which are roughly analogous to input and output units in feed-forward layers. But we also have to factor in the kernel size. Pytorch has a nice convenience function to help. . We can now package all of the code above, with some refactoring, into a function that automatically generates complex Rayleigh weights given an input pytorch module. . import torch def get_complex_inits(module, seed=None, criterion=&#39;he&#39;): &quot;Creates real and imaginary Rayleigh weights for initialization.&quot; # create random number generator rand = RandomState(seed if seed is None else torch.initial_seed()) # get shape of the weights weight_size = module.weight.size() # find number of input and output connection fan_in,fan_out = torch.nn.init._calculate_fan_in_and_fan_out(module.weight) # find sigma to meet chosen variance criteria assert criterion in (&#39;he&#39;,&#39;glorot&#39;) factor = fan_in if criterion == &#39;he&#39; else fan_in + fan_out sigma = 1. / np.sqrt(factor) # draw rayleigh magnitude samples magnitude = rand.rayleigh(scale=sigma, size=weight_size) # draw uniform angle samples phase = rand.uniform(low=-np.pi, high=np.pi, size=magnitude.shape) # split magnitudes into real and imaginary components real = magnitude * np.cos(phase) imag = magnitude * np.sin(phase) # turn into float tensors and return real,imag = map(torch.from_numpy, [real,imag]) return real,imag . Testing on a nn.Linear module . We can test this on a single layer using pytorch&#39;s nn.Linear module. . m = torch.nn.Linear(100, 50) real,imag = get_complex_inits(m) . Let&#39;s check if the weights are correctly distributed. Going back to our Rayleigh introduction, it is the magnitude that should be Rayleigh distributed. . # grab magnitude as flat vector of numpy samples magnitude = torch.sqrt(real**2 + imag**2).numpy().reshape(-1) . # pick points that cover sample range to compare with theoretical rayleigh pdf points = np.linspace(0, magnitude.max(), 1000) ray_pdf = rayleigh_pdf(points, sigma=1./np.sqrt(100)) # plot histogram of magnitudes vs. theoretical pdf plt.figure(figsize=(8,7)) plt.title(&#39;nn.Linear vs. Theoretical Rayleigh PDFs&#39;, fontsize=&#39;x-large&#39;) plt.xlabel(&#39;Sample Value&#39;, fontsize=&#39;x-large&#39;) plt.ylabel(&#39;Probability Density&#39;, fontsize=&#39;x-large&#39;) plt.hist(magnitude, density=True) plt.plot(points, ray_pdf, c=&#39;r&#39;, linewidth=3); . Success! . Testing on a nn.Conv2d module . What about a convolutional layer? Our main concern here is that both the tensor shape and fan_in/fan_out are handled correctly. . # make conv layer with 100 input features, 50 output features, and (3x3) kernel m = torch.nn.Conv2d(100, 50, 3) real,imag = get_complex_inits(m) # get the initial complex weights # make sure the shape of weights is ok print(f&#39;Shapes of real and imaginary convolution tensors: {real.shape}, {imag.shape}&#39;) . Shapes of real and imaginary convolution tensors: torch.Size([50, 100, 3, 3]), torch.Size([50, 100, 3, 3]) . Let&#39;s check if they are still Rayleigh distributed in the same way as before. . # grab magnitude as flat vector of numpy samples magnitude = torch.sqrt(real**2 + imag**2).numpy().reshape(-1) . # pick points that cover sample range to compare with theoretical rayleigh pdf points = np.linspace(0, magnitude.max(), 1000) # we need fan_in for the more complicated conv case fan_in,fan_out = torch.nn.init._calculate_fan_in_and_fan_out(m.weight) ray_pdf = rayleigh_pdf(points, sigma=1./np.sqrt(fan_in)) # plot histogram of magnitudes vs. theoretical pdf plt.figure(figsize=(8,7)) plt.title(&#39;nn.Conv2d vs. Theoretical Rayleigh PDFs&#39;, fontsize=&#39;x-large&#39;) plt.xlabel(&#39;Sample Value&#39;, fontsize=&#39;x-large&#39;) plt.ylabel(&#39;Probability Density&#39;, fontsize=&#39;x-large&#39;) plt.hist(magnitude, density=True) plt.plot(points, ray_pdf, c=&#39;r&#39;, linewidth=3); . Conclusion . In this post we created Rayleigh initializations for complex-valued neural networks. We started with an overview of the Rayleigh distribution. We then used this distribution to generate the magnitudes of complex weights. Next we added some phase information to randomly orient the vectors. After that, we made sure the weights matched a certain variance criteria to be good initializations. Lastly we put all that work together into a python function that returns pytorch tensors. The function as is not quite ready for use in a fully fledged complex-valued setup, but we will get there. . Part two will look at another type of complex initialization based on (semi) unitary matrixes. After that we will proceed to build complex convolutions and actvations. .",
            "url": "https://enzokro.dev/complex_networks/2020/07/20/Rayliegh-initialization-for-complex-networks.html",
            "relUrl": "/complex_networks/2020/07/20/Rayliegh-initialization-for-complex-networks.html",
            "date": " • Jul 20, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About",
          "content": "Researcher from Ecuador, working on RF applications with ML, speech, and video processing. .",
          "url": "https://enzokro.dev/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://enzokro.dev/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}