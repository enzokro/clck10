{
  
    
        "post0": {
            "title": "How to normalize spectrograms",
            "content": "Introduction . Spectrograms are often used as images to train deep networks for audio tasks. By treating spectrograms as images, we can borrow from the many powerful ideas in image recognition with deep learning. A spectrogram, however, is fundamentally different from natural images as we will see below. That brings up the central question of this post: how should spectrograms be normalized during training? . This post assumes some familiarity with deep learning and signal processing concepts like the FFT. It is also a light introduction to the fastaudio library. . Turning audio into an image . Image classification is a challenging task that was previously done with expert, handcrafted features. Now, features are automatically learned from labeled data instead. The success of these learned features has completely shifted the paradigm of Computer Vision. We would ideally like to apply these same, proven techniques to increase the performance of audio tasks. . However, audio is treated like a one dimensional signal in most Machine Learning applications. That means that raw audio is unusable with 2-D Convolutional Neural Networks (CNNs), which are the workhorses of modern image recognition. If we could somehow represent audio in two dimensions, like an image, then we could leverage the successful approaches in image classification. . Thankfully there are many ways of transforming audio into two dimensions. The most popular one is turning audio into a spectrogram. The spectrogram is a 2-D signal representation in time and frequency, so it can used with a 2-D CNN! But first we need some preprocessing to make training easier. . Unnormalized inputs can make it difficult for neural networks to learn well. We can solve this by normalizing the input features before passing them into a model. For natural images, normalization uses an estimated mean ($ mu$) and standard deviation ($ sigma$) as follows: . Subtract $ mu$ from the image values to give them a mean of $0$. | Divide the image values by $ sigma$ to give them a variance of $1$. | . In math terms, if $x$ is our image then $x_{ text{norm}}$ is: $$x_{ text{norm}} = frac{(x - mu)}{ sigma}$$ . Since spectrograms are so different from natural images, we should reevaluate if this type of normalization makes sense for them. . Why spectrograms are not images and how to normalize them . Now we can describe what makes spectrograms different from natural images. We start with a high-level description of images and their normalization and then do the same for spectrograms. A quick recap of the steps in the spectrogram process further shows how different they are from images. This recap naturally leads to a specific normalization strategy for spectrograms. Finally, we talk about Transfer Learning and why we avoid it in this post. . In an image, both axes (height and width) are in the spatial domain at the same scale. Images are stored as integers in the range of [0, 255]. To normalize them we first divide all pixels by 255, the max possible value, to map them into the range [0, 1]. Then, we find the statistics that approximately center the data with a mean of $0$ and a variance of $1$. The three RGB channels in a color image are normalized separately. If an image is greyscale then its single channel is normalized instead. . The axes in a spectrogram are from different domains than the axes in an image. In a spectrogram, the horizontal axis represents time and the vertical axis represents frequency. Each of these quantities has its own scale. The frequency dimension is determined by the size of the FFT window which also sets the spectral resolution. The time dimension is determined by the total length of the signal, the size of the FFT window, and the hop size of the window. You can check the documentation of the torch.stft function for more specifics on how these two axes are determined. . To be more specific, a spectrogram is actually the log of the power spectrum. Below we give a quick recap of how the spectrogram is computed to show how much it differs from images. . If $ text{x}$ is the input audio then the STFT returns the spectrum: $$ text{spectrum} = text{STFT(x)}$$ We are more interested in the energy or power of the signal, so we take the absolute value of the STFT and square it: $$ text{power_spectrum} = | text{STFT(x)}|^2$$ We cannot directly use the power spectrum as an input because it has a few strong peaks and many small values. For details on exactly why this is problem check this fantastic post that also deals with spectrogram normalization. Taking the the log of the power spectrum spreads out the values and makes them better features. This becomes the spectrogram: $$ text{spectrogram} = log(| text{STFT(x)}|^2)$$ The range of the log function is from $- infty$ to $+ infty$ which is clearly different than the integers from 0 to 255 in an image. . A spectrogram transformation can also be interpreted as a &quot;channelizer&quot;. That is a fancy way of saying it takes the continuous frequency spectrum of our signal and chops it up into discrete bins, or channels. For example, consider a signal sampled at 16 kHz where we take an STFT size of 512 bins. The spectrogram will have 512 channels where each one has a &quot;bandwidth&quot; of $$16 text{kHz} / 512 text{bins} = 31.25 text{Hz per bin}$$ . These spectrogram channels are clearly different from the image channels we are used to. So it raises the question: should we (or can we?) normalize the entire spectrogram &quot;image&quot; with a single, global value? Or should we normalize each spectrogram channel just like the channels in an image? In the rest of this post, we will compare a global and channel-based normalization for spectrograms to determine which is better. . Quick note on Transfer Learning . We also have to talk about Transfer Learning in the context of normalization. In Transfer Learning, it is best-practice to normalize the new dataset with the statistics from the old dataset. This makes sure that the new network inputs are at the same scale as the original inputs. Most pretrained vision models have been trained on ImageNet so we normalize new images with ImageNet statistics. We avoid Transfer Learning in this post and instead train an 18-layer ResNet from scratch. The reason is that pretrained image models have learned to operate at a completely different scale than spectrograms. And the main goal here is to learn our own scalings instead. . Downloading a sample dataset . To keep things practical, we will apply these spectrogram normalization techniques to a sound classification challenge hosted by fastaudio. fastaudio is a community extension of the fastai library to make audio tasks with neural networks more accessible. The challenge here is to classify sounds in the ESC-50 dataset, where ESC-50 stands for &quot;Environment Sound Classification with 50 classes&quot;. This dataset has many different types of sounds which show how varied audio spectrograms can be. . Many of the lines below are based on the fastaudio baseline results notebook. . Downloading the ESC-50 dataset . The first step is to download the data. ESC-50 is already included in fastaudio so we can grab it with untar_data. . from fastai.vision.all import * from fastaudio.core.all import * from fastaudio.augment.all import * # already in fastaudio, can download with fastai&#39;s `untar_data` path = untar_data(URLs.ESC50) . The downloaded audio files are inside the aptly named audio folder. Below we use the ls method, a fastai addition to python&#39;s pathlib.Path, to check the contents of this folder. . wavs = (path/&quot;audio&quot;).ls() wavs . (#2000) [Path(&#39;/home/titan2/2A/cck/fastai/data/master/audio/3-68630-A-40.wav&#39;),Path(&#39;/home/titan2/2A/cck/fastai/data/master/audio/5-260433-A-39.wav&#39;),Path(&#39;/home/titan2/2A/cck/fastai/data/master/audio/5-188796-A-45.wav&#39;),Path(&#39;/home/titan2/2A/cck/fastai/data/master/audio/1-57318-A-13.wav&#39;),Path(&#39;/home/titan2/2A/cck/fastai/data/master/audio/4-141365-A-18.wav&#39;),Path(&#39;/home/titan2/2A/cck/fastai/data/master/audio/1-9886-A-49.wav&#39;),Path(&#39;/home/titan2/2A/cck/fastai/data/master/audio/3-71964-C-4.wav&#39;),Path(&#39;/home/titan2/2A/cck/fastai/data/master/audio/5-201172-A-46.wav&#39;),Path(&#39;/home/titan2/2A/cck/fastai/data/master/audio/3-253084-E-2.wav&#39;),Path(&#39;/home/titan2/2A/cck/fastai/data/master/audio/1-91359-A-11.wav&#39;)...] . The output of ls shows 2,000 audio files. But the filenames are not very descriptive, so how do we know what is actually in each one? Thankfully, as with many datasets, the download includes a table with more information about the data (aka metadata). . # read the audio metadata and show the first few rows df = pd.read_csv(path/&quot;meta&quot;/&quot;esc50.csv&quot;) df.head() . filename fold target category esc10 src_file take . 0 1-100032-A-0.wav | 1 | 0 | dog | True | 100032 | A | . 1 1-100038-A-14.wav | 1 | 14 | chirping_birds | False | 100038 | A | . 2 1-100210-A-36.wav | 1 | 36 | vacuum_cleaner | False | 100210 | A | . 3 1-100210-B-36.wav | 1 | 36 | vacuum_cleaner | False | 100210 | B | . 4 1-101296-A-19.wav | 1 | 19 | thunderstorm | False | 101296 | A | . The key info from this table are in the filename and category columns. filename gives the name of a file inside of the audio folder. category tells us which class a file belongs to. . The last file in the data directory will be our working example for normalization. We can index into the metadata table above using this file&#39;s name to learn more about it. . # pick the row where &quot;filename&quot; matches the file&#39;s &quot;name&quot;. df.loc[df.filename == wavs[-1].name] . filename fold target category esc10 src_file take . 1826 5-216213-A-13.wav | 5 | 13 | crickets | False | 216213 | A | . This is a recording of crickets! We can load this file with the AudioTensor class in fastaudio. Its create function reads the audio samples straight into a torch.Tensor. . # create an AudioTensor from a file path sample = AudioTensor.create(wavs[-1]) . An AudioTensor can plot and even play the audio with its show method. . print(f&#39;Audio shape [channels, samples]: {sample.shape}&#39;) sample.show(); . Audio shape [channels, samples]: torch.Size([1, 220500]) . Your browser does not support the audio element. Each &quot;burst&quot; in the plot above is a cricket chirp. There are three full chirps and the early starts of a fourth chirp. . How to normalize audio waveforms . The first step is normalizing the audio waveform itself. We give it a mean of zero and unit variance in the usual way: . $$ text{norm_audio} = frac{ text{audio} - mean( text{audio})}{std( text{audio})} $$ . # normalize the waveform norm_sample = (sample - sample.mean()) / sample.std() . Let&#39;s check if the mean is roughly $0$ and the variance is roughly $1$: . # checking the mean print(f&#39;Original audio mean: {sample.mean()}&#39;) print(f&#39;Normalized audio mean: {norm_sample.mean()}&#39;) . Original audio mean: -1.781299215508625e-05 Normalized audio mean: 2.876160698495056e-10 . # checking the standard deviation print(f&#39;Original audio standard dev: {sample.var()}&#39;) print(f&#39;Normalized audio standard dev: {norm_sample.var()}&#39;) . Original audio standard dev: 0.008586421608924866 Normalized audio standard dev: 1.0 . Success! The waveform is normalized. . For convenience later on, we define the AudioNormalize transform to normalize waveforms in the fastai training loop. . class AudioNormalize(Transform): &quot;Normalizes a single `AudioTensor`.&quot; def encodes(self, x:AudioTensor): return (x-x.mean()) / x.std() . # checking if the Transform normalized the waveform wav_norm = AudioNormalize() norm_sample = wav_norm(sample) print(f&#39;Audio mean after transform: {norm_sample.mean()}&#39;) print(f&#39;Audio standard dev after transform: {norm_sample.var()}&#39;) . Audio mean after transform: 2.876160698495056e-10 Audio standard dev after transform: 1.0 . Converting audio into spectrograms . The next step is to extract a spectrogram from the normalized audio. We can do this with the AudioToSpec class in fastaudio. This class takes an AudioTensor as input and, as we might expect, returns an AudioSpectrogram. . # create a fastaudio Transform to convert audio into spectrograms cfg = AudioConfig.BasicSpectrogram() # with default torchaudio parameters audio2spec = AudioToSpec.from_cfg(cfg) # extract the spectrogram spec = audio2spec(norm_sample) . The show method of the AudioSpectrogram is a great, quick way to plot the spectrogram. . print(f&#39;Spectrogram shape [channels, bins, time_steps]: {spec.shape}&#39;) spec.show(); . Spectrogram shape [channels, bins, time_steps]: torch.Size([1, 201, 1103]) . The colorbar on the right showing the power in the signal is especially helpful since matplotlib always scales the values in a plot to the same color range. Without this colorbar, it is impossible to know or even guess the specific values in a spectrogram plot. . Getting spectrogram normalization stats . To get the normalization stats, we have to step through the training set and find the mean and standard deviation of each mini-batch. Then we average all the mini-batch statistics to get a single pair of ($ mu, sigma)$ normalization statistics. Note that normalization statistics always come from the training set. This is a crucial place to avoid data leakage. One small detail: if your training dataset is large enough it is not necessary to go through the whole set. Sampling 10% to 20% of the dataset can be enough for accurate statistics. However, since ESC-50 is small we find ($ mu, sigma)$ from the whole set. . To accumulate these statistics over mini-batches we can borrow and slightly refactor a class from this very helpful post. The StatsRecorder class below tracks the mean and standard deviation across mini-batches. . class StatsRecorder: def __init__(self, red_dims=(0,2,3)): &quot;&quot;&quot;Accumulates normalization statistics across mini-batches. ref: http://notmatthancock.github.io/2017/03/23/simple-batch-stat-updates.html &quot;&quot;&quot; self.red_dims = red_dims # which mini-batch dimensions to average over self.nobservations = 0 # running number of observations def update(self, data): &quot;&quot;&quot; data: ndarray, shape (nobservations, ndimensions) &quot;&quot;&quot; # initialize stats and dimensions on first batch if self.nobservations == 0: self.mean = data.mean(dim=self.red_dims, keepdim=True) self.std = data.std (dim=self.red_dims,keepdim=True) self.nobservations = data.shape[0] self.ndimensions = data.shape[1] else: if data.shape[1] != self.ndimensions: raise ValueError(&#39;Data dims do not match previous observations.&#39;) # find mean of new mini batch newmean = data.mean(dim=self.red_dims, keepdim=True) newstd = data.std(dim=self.red_dims, keepdim=True) # update number of observations m = self.nobservations * 1.0 n = data.shape[0] # update running statistics tmp = self.mean self.mean = m/(m+n)*tmp + n/(m+n)*newmean self.std = m/(m+n)*self.std**2 + n/(m+n)*newstd**2 + m*n/(m+n)**2 * (tmp - newmean)**2 self.std = torch.sqrt(self.std) # update total number of seen samples self.nobservations += n . By default StatsRecorder averages over the image channel dimensions (grayscale or RGB). The red_dims argument might look familiar from normalization code in other Computer Vision tasks (also the Normalize in fastai). To average over spectrogram channels instead we only need to pass a different red_dims. . Building the dataset loader . The setup belows follows the fastaudio ESC-50 baseline to iterate through the training dataset. It is worth mentioning that the files in ESC-50 are sampled 44.1 kHz, but fastaudio will resample them to 16 kHz by default. Downsampling like this risks throwing away some information. But, keeping the higher sampling rate almost triples the &quot;width&quot; (aka time) of the spectrogram. This larger image will take up more memory in the GPU and limits our batch size and architecture choices. We keep this downsampling since it gives the spectrograms a very reasonable shape of [201, 401], compared with the much larger shape of [201, 1103] if we don&#39;t downsample. . def CrossValidationSplitter(col=&#39;fold&#39;, fold=1): &quot;Split `items` (supposed to be a dataframe) by fold in `col`&quot; def _inner(o): assert isinstance(o, pd.DataFrame), &quot;ColSplitter only works when your items are a pandas DataFrame&quot; col_values = o.iloc[:,col] if isinstance(col, int) else o[col] valid_idx = (col_values == fold).values.astype(&#39;bool&#39;) return IndexSplitter(mask2idxs(valid_idx))(o) return _inner auds = DataBlock(blocks=(AudioBlock, CategoryBlock), get_x=ColReader(&quot;filename&quot;, pref=path/&quot;audio&quot;), splitter=CrossValidationSplitter(fold=1), item_tfms = [AudioNormalize], batch_tfms = [audio2spec], get_y=ColReader(&quot;category&quot;)) dbunch = auds.dataloaders(df, bs=64) dbunch.show_batch(figsize=(7,7)) . Finding the statistics . Below we make two recorders: one for global statistics and the other for channel-based statistics. Then we step through the training dataset to find both sets of stats. . # create recorders global_stats = StatsRecorder() channel_stats = StatsRecorder(red_dims=(0,1,3)) # step through the training dataset with torch.no_grad(): for idx,(x,y) in enumerate(iter(dbunch.train)): # update normalization statistics global_stats.update(x) channel_stats.update(x) # parse out both sets of stats global_mean,global_std = global_stats.mean,global_stats.std channel_mean,channel_std = channel_stats.mean,channel_stats.std . We can check the shape of the statistics to make sure they are correct. For the global statistics, we expect a shape of: [1,1,1,1]. With spectrogram channel normalizations, we expect one value per spectrogram bin for a shape of [1,1,201,1]. . print(f&#39;Shape of global mean: {global_mean.shape}&#39;) print(f&#39;Shape of global standard dev: {global_std.shape}&#39;) . Shape of global mean: torch.Size([1, 1, 1, 1]) Shape of global standard dev: torch.Size([1, 1, 1, 1]) . print(f&#39;Shape of channel mean: {channel_mean.shape}&#39;) print(f&#39;Shape of channel standard dev: {channel_std.shape}&#39;) . Shape of channel mean: torch.Size([1, 1, 201, 1]) Shape of channel standard dev: torch.Size([1, 1, 201, 1]) . Transforms to normalize mini-batches . To use these normalization statistics in a fastai training loop we need to extend the Normalize class. The reason is type dispatch. fastai normalization uses ImageNet statistics due to the focus on transfer learning with color images. But this ImageNet normalization is only applied on RGB images of the TensorImage class, while AudioSpectrogram subclasses the different TensorImageBase. The solution is to define encodes and decodes for TensorImageBase instead. . class SpecNormalize(Normalize): &quot;Normalize/denorm batch of `TensorImage`&quot; def encodes(self, x:TensorImageBase): return (x-self.mean) / self.std def decodes(self, x:TensorImageBase): f = to_cpu if x.device.type==&#39;cpu&#39; else noop return (x*f(self.std) + f(self.mean)) . # make global and channel normalizers GlobalSpecNorm = SpecNormalize(global_mean, global_std, axes=(0,2,3)) ChannelSpecNorm = SpecNormalize(channel_mean, channel_std, axes=(0,1,3)) . Training with normalizations . Now for the moment of truth. We train with the two different spectrogram normalizations and measure their impact. For this we again follow the fastaudio baseline and train each type of normalization for 20 epochs. The final score is the averaged accuracy of five runs. . epochs = 20 num_runs = 5 . Training helpers . To avoid repeating ourselves, the helper functions below build the dataloaders and run the training loops. The get_dls function makes it clear which normalization is being applied. The train_loops function repeats training runs a given number of times. . def get_dls(bs=64, item_tfms=[], batch_tfms=[]): &quot;Get dataloaders with given `bs` and batch/item tfms.&quot; auds = DataBlock(blocks=(AudioBlock, CategoryBlock), get_x=ColReader(&quot;filename&quot;, pref=path/&quot;audio&quot;), splitter=CrossValidationSplitter(fold=1), item_tfms=item_tfms, # for waveform normalization batch_tfms=batch_tfms, # for spectrogram normalization get_y=ColReader(&quot;category&quot;)) dls = auds.dataloaders(df, bs=bs) return dls def make_xresnet_grayscale(model, n_in=1): &quot;Modifies xresnet `model` for single channel images.&quot; model[0][0].in_channels = n_in # average weights to reduce dimension model[0][0].weight = torch.nn.parameter.Parameter(model[0][0].weight.mean(1, keepdim=True)) def train_loops(dls, name, num_runs=num_runs, epochs=epochs): &quot;Runs `num_runs` training loops with `dls` for given `epochs`.&quot; accuracies = [] for i in range(num_runs): # make new grayscale xresnet model = xresnet18(pretrained=False, n_out=50) make_xresnet_grayscale(model, n_in=1) # get learner for this run learn = Learner(dls, model, metrics=[accuracy]) # train network and track accuracy learn.fit_one_cycle(epochs) accuracies.append(learn.recorder.values[-1][-1]) print(f&#39;Average accuracy for &quot;{name}&quot;: {sum(accuracies) / num_runs}&#39;) . Baseline performance . Before getting carried away with normalization, we have to first set a baseline without normalizations. This allows us to evaluate the impact of normalization later on, else there is no way to know if normalization helps at all. . # data without normalization dls = get_dls(batch_tfms=[audio2spec]) # run training loops train_loops(dls, name=&#39;No Norm&#39;) . epoch train_loss valid_loss accuracy time . 0 | 3.969248 | 3.820465 | 0.047500 | 00:12 | . 1 | 3.712643 | 3.445267 | 0.117500 | 00:10 | . 2 | 3.441750 | 3.214144 | 0.135000 | 00:10 | . 3 | 3.175456 | 2.874947 | 0.237500 | 00:10 | . 4 | 2.918222 | 2.914521 | 0.257500 | 00:10 | . 5 | 2.663215 | 2.555236 | 0.267500 | 00:10 | . 6 | 2.407773 | 2.380169 | 0.367500 | 00:10 | . 7 | 2.180136 | 2.734919 | 0.350000 | 00:10 | . 8 | 1.932248 | 2.028967 | 0.440000 | 00:10 | . 9 | 1.694431 | 2.054369 | 0.435000 | 00:10 | . 10 | 1.491780 | 1.711151 | 0.535000 | 00:09 | . 11 | 1.298917 | 1.441046 | 0.577500 | 00:10 | . 12 | 1.134486 | 1.482220 | 0.612500 | 00:10 | . 13 | 0.977481 | 1.278379 | 0.662500 | 00:10 | . 14 | 0.842470 | 1.215223 | 0.670000 | 00:10 | . 15 | 0.729177 | 1.147078 | 0.685000 | 00:10 | . 16 | 0.640120 | 1.074852 | 0.692500 | 00:10 | . 17 | 0.573704 | 1.070244 | 0.690000 | 00:10 | . 18 | 0.523956 | 1.047121 | 0.712500 | 00:10 | . 19 | 0.489059 | 1.049863 | 0.700000 | 00:10 | . epoch train_loss valid_loss accuracy time . 0 | 4.095797 | 3.869291 | 0.050000 | 00:10 | . 1 | 3.801821 | 3.425221 | 0.117500 | 00:10 | . 2 | 3.496061 | 3.167158 | 0.185000 | 00:10 | . 3 | 3.199899 | 3.019003 | 0.175000 | 00:10 | . 4 | 2.915351 | 2.768171 | 0.252500 | 00:10 | . 5 | 2.640109 | 2.538733 | 0.312500 | 00:10 | . 6 | 2.361073 | 2.282025 | 0.357500 | 00:10 | . 7 | 2.088267 | 2.159701 | 0.420000 | 00:10 | . 8 | 1.831442 | 1.913155 | 0.457500 | 00:10 | . 9 | 1.597652 | 1.653123 | 0.532500 | 00:10 | . 10 | 1.390415 | 1.562032 | 0.600000 | 00:10 | . 11 | 1.206014 | 1.437033 | 0.585000 | 00:10 | . 12 | 1.039442 | 1.518623 | 0.552500 | 00:10 | . 13 | 0.890473 | 1.226299 | 0.642500 | 00:10 | . 14 | 0.772993 | 1.225722 | 0.657500 | 00:10 | . 15 | 0.662319 | 1.070868 | 0.697500 | 00:10 | . 16 | 0.575195 | 1.064739 | 0.700000 | 00:10 | . 17 | 0.504618 | 1.009349 | 0.715000 | 00:10 | . 18 | 0.454480 | 1.002710 | 0.717500 | 00:10 | . 19 | 0.422400 | 1.000769 | 0.725000 | 00:10 | . epoch train_loss valid_loss accuracy time . 0 | 3.971367 | 3.820944 | 0.067500 | 00:10 | . 1 | 3.723815 | 3.436206 | 0.102500 | 00:10 | . 2 | 3.439669 | 3.116783 | 0.182500 | 00:10 | . 3 | 3.146865 | 2.959616 | 0.182500 | 00:10 | . 4 | 2.867068 | 3.017721 | 0.227500 | 00:10 | . 5 | 2.587413 | 2.564640 | 0.297500 | 00:10 | . 6 | 2.331449 | 2.480428 | 0.370000 | 00:10 | . 7 | 2.083794 | 2.047217 | 0.430000 | 00:10 | . 8 | 1.853696 | 1.991711 | 0.400000 | 00:10 | . 9 | 1.636101 | 1.766080 | 0.475000 | 00:10 | . 10 | 1.416112 | 1.637351 | 0.475000 | 00:10 | . 11 | 1.228323 | 1.433632 | 0.592500 | 00:10 | . 12 | 1.056026 | 1.377866 | 0.612500 | 00:10 | . 13 | 0.909199 | 1.266701 | 0.657500 | 00:10 | . 14 | 0.783041 | 1.179326 | 0.655000 | 00:10 | . 15 | 0.672315 | 1.102665 | 0.685000 | 00:10 | . 16 | 0.584509 | 1.058187 | 0.697500 | 00:10 | . 17 | 0.512352 | 1.008986 | 0.722500 | 00:10 | . 18 | 0.470781 | 1.001468 | 0.715000 | 00:10 | . 19 | 0.431511 | 0.997061 | 0.720000 | 00:10 | . epoch train_loss valid_loss accuracy time . 0 | 3.994381 | 3.816283 | 0.040000 | 00:10 | . 1 | 3.751954 | 3.452930 | 0.100000 | 00:10 | . 2 | 3.493272 | 3.175919 | 0.155000 | 00:10 | . 3 | 3.209845 | 3.087182 | 0.187500 | 00:10 | . 4 | 2.924385 | 2.775273 | 0.195000 | 00:10 | . 5 | 2.629113 | 2.482365 | 0.307500 | 00:10 | . 6 | 2.349416 | 2.483069 | 0.332500 | 00:10 | . 7 | 2.091504 | 2.187751 | 0.367500 | 00:10 | . 8 | 1.825146 | 2.021458 | 0.457500 | 00:10 | . 9 | 1.585074 | 1.668301 | 0.505000 | 00:10 | . 10 | 1.382548 | 1.574356 | 0.582500 | 00:10 | . 11 | 1.197515 | 1.477407 | 0.610000 | 00:10 | . 12 | 1.029958 | 1.295299 | 0.627500 | 00:10 | . 13 | 0.880848 | 1.173190 | 0.662500 | 00:10 | . 14 | 0.761054 | 1.202786 | 0.645000 | 00:10 | . 15 | 0.652563 | 1.075612 | 0.697500 | 00:10 | . 16 | 0.565155 | 1.020114 | 0.697500 | 00:10 | . 17 | 0.499538 | 0.984256 | 0.710000 | 00:10 | . 18 | 0.451551 | 0.967878 | 0.710000 | 00:10 | . 19 | 0.415858 | 0.967541 | 0.712500 | 00:10 | . epoch train_loss valid_loss accuracy time . 0 | 4.050342 | 3.846032 | 0.065000 | 00:10 | . 1 | 3.785083 | 3.463626 | 0.115000 | 00:10 | . 2 | 3.504684 | 3.193883 | 0.155000 | 00:11 | . 3 | 3.218580 | 3.018942 | 0.177500 | 00:10 | . 4 | 2.920237 | 2.771982 | 0.217500 | 00:11 | . 5 | 2.635390 | 2.622943 | 0.300000 | 00:10 | . 6 | 2.387567 | 2.534594 | 0.315000 | 00:10 | . 7 | 2.146565 | 2.396223 | 0.352500 | 00:11 | . 8 | 1.926011 | 1.926806 | 0.437500 | 00:10 | . 9 | 1.686316 | 1.878930 | 0.475000 | 00:10 | . 10 | 1.475129 | 1.967822 | 0.435000 | 00:11 | . 11 | 1.293371 | 1.489895 | 0.580000 | 00:11 | . 12 | 1.123488 | 1.452433 | 0.600000 | 00:11 | . 13 | 0.977898 | 1.422307 | 0.617500 | 00:10 | . 14 | 0.853223 | 1.229743 | 0.635000 | 00:10 | . 15 | 0.734233 | 1.123997 | 0.677500 | 00:10 | . 16 | 0.640662 | 1.114177 | 0.675000 | 00:10 | . 17 | 0.568609 | 1.098119 | 0.682500 | 00:10 | . 18 | 0.520709 | 1.075979 | 0.697500 | 00:10 | . 19 | 0.482832 | 1.075474 | 0.697500 | 00:10 | . Average accuracy for &#34;No Norm&#34;: 0.7110000014305115 . Performance with global statistics . Next we normalize each audio waveform and the spectrograms with global, scalar statistics. . # data with waveform and global normalization dls = get_dls(item_tfms=[AudioNormalize], batch_tfms=[audio2spec, GlobalSpecNorm]) # run training loops train_loops(dls, name=&#39;Global Norm&#39;) . epoch train_loss valid_loss accuracy time . 0 | 4.026500 | 4.044263 | 0.032500 | 00:10 | . 1 | 3.786512 | 3.503887 | 0.105000 | 00:10 | . 2 | 3.517005 | 3.151405 | 0.197500 | 00:10 | . 3 | 3.212996 | 2.836708 | 0.217500 | 00:11 | . 4 | 2.892882 | 2.597347 | 0.260000 | 00:10 | . 5 | 2.568272 | 2.591951 | 0.292500 | 00:10 | . 6 | 2.249284 | 2.132349 | 0.402500 | 00:11 | . 7 | 1.986435 | 2.256171 | 0.377500 | 00:10 | . 8 | 1.719509 | 1.995517 | 0.440000 | 00:10 | . 9 | 1.473958 | 1.578709 | 0.545000 | 00:10 | . 10 | 1.270131 | 1.502453 | 0.585000 | 00:10 | . 11 | 1.076702 | 1.462597 | 0.617500 | 00:10 | . 12 | 0.909848 | 1.378287 | 0.635000 | 00:10 | . 13 | 0.765447 | 1.226909 | 0.687500 | 00:10 | . 14 | 0.645765 | 1.223053 | 0.660000 | 00:10 | . 15 | 0.543597 | 1.093059 | 0.710000 | 00:11 | . 16 | 0.463450 | 1.078949 | 0.717500 | 00:10 | . 17 | 0.403010 | 1.034981 | 0.730000 | 00:10 | . 18 | 0.364191 | 1.027838 | 0.725000 | 00:10 | . 19 | 0.333268 | 1.031052 | 0.725000 | 00:10 | . epoch train_loss valid_loss accuracy time . 0 | 4.015247 | 4.054859 | 0.015000 | 00:10 | . 1 | 3.765068 | 3.477965 | 0.112500 | 00:10 | . 2 | 3.489605 | 3.153201 | 0.212500 | 00:10 | . 3 | 3.185047 | 2.838492 | 0.210000 | 00:10 | . 4 | 2.876298 | 3.269031 | 0.187500 | 00:10 | . 5 | 2.564873 | 2.569763 | 0.287500 | 00:10 | . 6 | 2.283852 | 2.240474 | 0.417500 | 00:10 | . 7 | 2.036773 | 2.255483 | 0.332500 | 00:10 | . 8 | 1.768668 | 1.871785 | 0.510000 | 00:10 | . 9 | 1.535749 | 1.944937 | 0.505000 | 00:10 | . 10 | 1.330370 | 1.748090 | 0.520000 | 00:10 | . 11 | 1.153153 | 1.426051 | 0.625000 | 00:10 | . 12 | 0.972430 | 1.437676 | 0.615000 | 00:10 | . 13 | 0.819660 | 1.192760 | 0.650000 | 00:11 | . 14 | 0.703579 | 1.205461 | 0.675000 | 00:10 | . 15 | 0.598147 | 1.189074 | 0.680000 | 00:10 | . 16 | 0.514990 | 1.061301 | 0.717500 | 00:10 | . 17 | 0.448104 | 1.036746 | 0.720000 | 00:10 | . 18 | 0.399913 | 1.020626 | 0.727500 | 00:11 | . 19 | 0.370449 | 1.023248 | 0.725000 | 00:10 | . epoch train_loss valid_loss accuracy time . 0 | 4.009436 | 4.171368 | 0.025000 | 00:10 | . 1 | 3.771747 | 3.559254 | 0.082500 | 00:10 | . 2 | 3.491799 | 3.139065 | 0.152500 | 00:10 | . 3 | 3.189236 | 2.902275 | 0.212500 | 00:10 | . 4 | 2.851417 | 2.510654 | 0.320000 | 00:10 | . 5 | 2.535454 | 2.469320 | 0.325000 | 00:10 | . 6 | 2.232145 | 2.299582 | 0.405000 | 00:10 | . 7 | 1.960728 | 2.387277 | 0.402500 | 00:10 | . 8 | 1.700888 | 1.779266 | 0.505000 | 00:10 | . 9 | 1.472246 | 1.544506 | 0.550000 | 00:10 | . 10 | 1.266052 | 1.389619 | 0.597500 | 00:10 | . 11 | 1.084707 | 1.309642 | 0.625000 | 00:10 | . 12 | 0.923691 | 1.280443 | 0.655000 | 00:11 | . 13 | 0.773234 | 1.232845 | 0.672500 | 00:10 | . 14 | 0.653177 | 1.112382 | 0.662500 | 00:10 | . 15 | 0.548954 | 1.101867 | 0.687500 | 00:11 | . 16 | 0.469678 | 1.043254 | 0.697500 | 00:10 | . 17 | 0.408952 | 1.030811 | 0.717500 | 00:10 | . 18 | 0.370070 | 1.006194 | 0.735000 | 00:10 | . 19 | 0.339324 | 1.007815 | 0.740000 | 00:10 | . epoch train_loss valid_loss accuracy time . 0 | 4.062994 | 4.126382 | 0.022500 | 00:10 | . 1 | 3.816543 | 3.583234 | 0.100000 | 00:10 | . 2 | 3.538556 | 3.187394 | 0.175000 | 00:10 | . 3 | 3.237335 | 2.953974 | 0.200000 | 00:10 | . 4 | 2.950168 | 2.901750 | 0.220000 | 00:10 | . 5 | 2.663354 | 2.498471 | 0.340000 | 00:10 | . 6 | 2.388274 | 2.204422 | 0.435000 | 00:11 | . 7 | 2.118268 | 2.354115 | 0.415000 | 00:10 | . 8 | 1.855800 | 1.905706 | 0.502500 | 00:10 | . 9 | 1.604701 | 1.698043 | 0.517500 | 00:10 | . 10 | 1.384119 | 1.651268 | 0.540000 | 00:10 | . 11 | 1.191743 | 1.561539 | 0.585000 | 00:10 | . 12 | 1.018403 | 1.375490 | 0.655000 | 00:10 | . 13 | 0.868584 | 1.236311 | 0.675000 | 00:10 | . 14 | 0.740142 | 1.200392 | 0.687500 | 00:10 | . 15 | 0.637373 | 1.123281 | 0.685000 | 00:10 | . 16 | 0.553846 | 1.127196 | 0.705000 | 00:10 | . 17 | 0.486062 | 1.091728 | 0.712500 | 00:11 | . 18 | 0.441612 | 1.089931 | 0.720000 | 00:10 | . 19 | 0.408501 | 1.084071 | 0.717500 | 00:11 | . epoch train_loss valid_loss accuracy time . 0 | 4.028913 | 4.233477 | 0.040000 | 00:10 | . 1 | 3.771104 | 3.508428 | 0.075000 | 00:10 | . 2 | 3.471879 | 3.133292 | 0.172500 | 00:10 | . 3 | 3.169908 | 3.090198 | 0.185000 | 00:10 | . 4 | 2.840620 | 2.754784 | 0.265000 | 00:10 | . 5 | 2.521406 | 2.394690 | 0.367500 | 00:10 | . 6 | 2.214809 | 2.023241 | 0.485000 | 00:10 | . 7 | 1.920039 | 1.977669 | 0.432500 | 00:10 | . 8 | 1.659176 | 1.723857 | 0.557500 | 00:10 | . 9 | 1.427005 | 1.615716 | 0.555000 | 00:10 | . 10 | 1.215173 | 1.503057 | 0.585000 | 00:10 | . 11 | 1.024808 | 1.612414 | 0.555000 | 00:10 | . 12 | 0.859482 | 1.484262 | 0.590000 | 00:10 | . 13 | 0.724764 | 1.237992 | 0.650000 | 00:11 | . 14 | 0.599130 | 1.137059 | 0.700000 | 01:21 | . 15 | 0.501238 | 1.028849 | 0.730000 | 01:32 | . 16 | 0.427350 | 0.991878 | 0.742500 | 01:13 | . 17 | 0.371157 | 0.966464 | 0.740000 | 00:13 | . 18 | 0.329858 | 0.954331 | 0.750000 | 00:10 | . 19 | 0.302154 | 0.953994 | 0.750000 | 00:10 | . Average accuracy for &#34;Global Norm&#34;: 0.7315000057220459 . Performance with channel statistics . Finally, we normalize each audio waveform and the spectrograms with channel-based statistics. . # get data with waveform and channel normalization dls = get_dls(item_tfms=[AudioNormalize], batch_tfms=[audio2spec, ChannelSpecNorm]) # run training loops train_loops(dls, name=&#39;Channel Norm&#39;) . epoch train_loss valid_loss accuracy time . 0 | 3.969699 | 4.032131 | 0.017500 | 00:10 | . 1 | 3.730814 | 3.499502 | 0.077500 | 00:10 | . 2 | 3.474650 | 3.243829 | 0.172500 | 00:10 | . 3 | 3.231099 | 3.049371 | 0.205000 | 00:10 | . 4 | 2.968587 | 3.039508 | 0.195000 | 00:10 | . 5 | 2.697122 | 2.456799 | 0.325000 | 00:10 | . 6 | 2.433818 | 2.410477 | 0.355000 | 00:10 | . 7 | 2.190638 | 2.454575 | 0.342500 | 00:10 | . 8 | 1.952075 | 1.899792 | 0.472500 | 00:10 | . 9 | 1.715165 | 1.893039 | 0.502500 | 00:10 | . 10 | 1.535082 | 2.375675 | 0.442500 | 00:10 | . 11 | 1.356122 | 1.683109 | 0.552500 | 00:10 | . 12 | 1.181517 | 1.433426 | 0.627500 | 00:11 | . 13 | 1.017601 | 1.329792 | 0.655000 | 00:10 | . 14 | 0.873645 | 1.263948 | 0.657500 | 00:10 | . 15 | 0.761042 | 1.279020 | 0.682500 | 00:10 | . 16 | 0.673481 | 1.179939 | 0.672500 | 00:11 | . 17 | 0.610184 | 1.155781 | 0.687500 | 00:10 | . 18 | 0.556256 | 1.152489 | 0.705000 | 00:11 | . 19 | 0.522203 | 1.144921 | 0.702500 | 00:11 | . epoch train_loss valid_loss accuracy time . 0 | 4.054279 | 4.112432 | 0.030000 | 00:11 | . 1 | 3.770565 | 3.521419 | 0.097500 | 00:10 | . 2 | 3.469904 | 3.091036 | 0.172500 | 00:11 | . 3 | 3.143619 | 2.904781 | 0.240000 | 00:11 | . 4 | 2.809256 | 2.658845 | 0.275000 | 00:11 | . 5 | 2.500878 | 2.275773 | 0.407500 | 00:11 | . 6 | 2.210242 | 2.663011 | 0.307500 | 00:11 | . 7 | 1.944063 | 2.182807 | 0.405000 | 00:11 | . 8 | 1.715874 | 1.783302 | 0.492500 | 00:11 | . 9 | 1.494896 | 1.829541 | 0.505000 | 00:11 | . 10 | 1.303766 | 1.607981 | 0.560000 | 00:11 | . 11 | 1.127682 | 1.706913 | 0.497500 | 00:12 | . 12 | 0.978785 | 1.558262 | 0.600000 | 00:12 | . 13 | 0.842449 | 1.248074 | 0.642500 | 00:12 | . 14 | 0.720391 | 1.149852 | 0.690000 | 00:11 | . 15 | 0.622296 | 1.122397 | 0.675000 | 00:11 | . 16 | 0.540070 | 1.051975 | 0.717500 | 00:12 | . 17 | 0.474873 | 1.022141 | 0.712500 | 00:11 | . 18 | 0.428939 | 1.019800 | 0.722500 | 00:11 | . 19 | 0.393752 | 1.007815 | 0.730000 | 00:12 | . epoch train_loss valid_loss accuracy time . 0 | 4.000810 | 4.160871 | 0.035000 | 00:12 | . 1 | 3.732802 | 3.468769 | 0.092500 | 00:13 | . 2 | 3.454084 | 3.086493 | 0.182500 | 00:14 | . 3 | 3.150999 | 3.153290 | 0.195000 | 00:25 | . 4 | 2.848003 | 2.756796 | 0.255000 | 00:49 | . 5 | 2.549159 | 2.368085 | 0.382500 | 01:09 | . 6 | 2.252295 | 2.275773 | 0.370000 | 01:06 | . 7 | 1.985005 | 1.918476 | 0.462500 | 00:21 | . 8 | 1.751421 | 2.049885 | 0.470000 | 00:18 | . 9 | 1.527885 | 1.697186 | 0.520000 | 00:18 | . 10 | 1.323436 | 1.694885 | 0.537500 | 00:22 | . 11 | 1.147804 | 1.428171 | 0.635000 | 01:16 | . 12 | 0.983308 | 1.369227 | 0.635000 | 01:11 | . 13 | 0.837482 | 1.243835 | 0.667500 | 01:15 | . 14 | 0.716261 | 1.197568 | 0.665000 | 01:01 | . 15 | 0.616029 | 1.104130 | 0.705000 | 00:36 | . 16 | 0.537511 | 1.080298 | 0.705000 | 01:11 | . 17 | 0.472788 | 1.067901 | 0.710000 | 00:27 | . 18 | 0.424530 | 1.053772 | 0.715000 | 00:19 | . 19 | 0.399455 | 1.050808 | 0.717500 | 00:24 | . epoch train_loss valid_loss accuracy time . 0 | 3.918051 | 4.025037 | 0.042500 | 00:37 | . 1 | 3.684587 | 3.529938 | 0.067500 | 01:00 | . 2 | 3.419808 | 3.125223 | 0.182500 | 00:45 | . 3 | 3.131472 | 3.015343 | 0.175000 | 00:40 | . 4 | 2.819206 | 2.528827 | 0.300000 | 00:25 | . 5 | 2.517766 | 2.448646 | 0.317500 | 00:13 | . 6 | 2.233410 | 2.906568 | 0.247500 | 00:11 | . 7 | 1.965338 | 2.038978 | 0.430000 | 00:11 | . 8 | 1.724016 | 1.756567 | 0.517500 | 00:11 | . 9 | 1.485556 | 2.177741 | 0.477500 | 00:11 | . 10 | 1.306133 | 1.772524 | 0.477500 | 00:10 | . 11 | 1.134276 | 1.347214 | 0.610000 | 00:11 | . 12 | 0.980466 | 1.378182 | 0.625000 | 00:10 | . 13 | 0.833448 | 1.186836 | 0.667500 | 00:11 | . 14 | 0.713612 | 1.131924 | 0.667500 | 00:11 | . 15 | 0.610140 | 1.131178 | 0.647500 | 00:11 | . 16 | 0.532159 | 1.099220 | 0.675000 | 00:11 | . 17 | 0.475023 | 1.056823 | 0.692500 | 00:11 | . 18 | 0.432302 | 1.044158 | 0.707500 | 00:11 | . 19 | 0.403398 | 1.036176 | 0.705000 | 00:11 | . epoch train_loss valid_loss accuracy time . 0 | 3.953691 | 4.050189 | 0.030000 | 00:10 | . 1 | 3.683712 | 3.337167 | 0.132500 | 00:11 | . 2 | 3.373129 | 3.000274 | 0.215000 | 00:10 | . 3 | 3.073662 | 2.747640 | 0.247500 | 00:11 | . 4 | 2.768026 | 2.885627 | 0.215000 | 00:11 | . 5 | 2.473330 | 2.373895 | 0.375000 | 00:11 | . 6 | 2.200260 | 2.475523 | 0.372500 | 00:10 | . 7 | 1.933803 | 1.945074 | 0.467500 | 00:11 | . 8 | 1.697937 | 1.811188 | 0.522500 | 00:11 | . 9 | 1.477577 | 2.002481 | 0.495000 | 00:11 | . 10 | 1.278312 | 1.651548 | 0.560000 | 00:10 | . 11 | 1.101771 | 1.621976 | 0.570000 | 00:11 | . 12 | 0.942633 | 1.415608 | 0.605000 | 00:10 | . 13 | 0.806072 | 1.334597 | 0.622500 | 00:11 | . 14 | 0.691570 | 1.245835 | 0.657500 | 00:11 | . 15 | 0.598590 | 1.120105 | 0.692500 | 00:11 | . 16 | 0.513867 | 1.121796 | 0.695000 | 00:11 | . 17 | 0.452095 | 1.074298 | 0.712500 | 00:10 | . 18 | 0.403653 | 1.055097 | 0.715000 | 00:11 | . 19 | 0.370189 | 1.040890 | 0.717500 | 00:11 | . Average accuracy for &#34;Channel Norm&#34;: 0.7144999861717224 . Discussion . I ran the cells above several times to make sure the patterns held. . Overall, there is a gain from global normalization. Channel-based normalization shows a much smaller benefit. While these increases in performance are small, there are several explanations for this that point us towards other approaches. . For starters, the spectrograms in ESC-50 are very different both within and across classes. In other words the activity in each spectrogram channel changes a lot from sample to sample. A global statistic likely fares better under these unpredictable conditions. If all the audio came from a similar source, like speech, then the per-channel normalization might fare better. . We also process the entire five second files at once, which is a large analysis window by audio standards. This large window means that each sample looks exactly the same in every epoch. If we used a smaller analysis window, say 2 seconds, we could randomly &quot;crop&quot; many spectrogram regions from a single example as a kind of data augmentation. The risk here is grabbing a silent region without any information but still giving it a class label (though an energy threshold can prevent this). Cropping with a smaller analysis window is one way to expose the networks to more samples and variability. . Using the entire waveform at once also means that the waveform statistics need to model a very long-term relationship. Going back to the cricket recording example: we would not expect good normalization statistics for the chirps to be the same as good statistics for the pauses in between chirps. To counter this it is possible to do a &quot;short-time&quot; normalization. Here we pick a sliding window, often much smaller than the whole waveform, and only normalize the data inside as it steps through the waveform. This &quot;short-time&quot; normalization can be applied with or without the global waveform normalization. . Furthermore, the spectrogram is a high-dimensional feature with 201 frequency bins. It is common in audio tasks to reduce this dimension by combining nearby bins. This is done with something called &quot;filterbanks&quot; which usually operate at the Mel frequency scale. This tutorial is one of my favorites and gives an incredibly clear description of Mel frequency and the filterbank process. There are other options such as Gammatone filterbanks as well. While this might seem like an expert handcrafted feature, there is good reason for using filterbanks in audio tasks. If we feed in a raw spectrogram, the early convolutional layers tend to learn something like a filterbank anyway! So directly feeding a filterbank into the network lets it focus on more complicated relationships. . We are also training a powerful 18-layer model from scratch with only 1600 images. While deep learning can handle datasets this small, it is usually only through Transfer Learning. But, we stayed away from Transfer Learning because pretrained networks are tightly coupled to their original dataset&#39;s normalization statistics. And the main idea here was to learn our own spectrogram scalings. It is possible that a smaller, simpler network will perform better. Looking at the training logs above, it seems the validation loss was still decreasing. So we&#39;d still have to train for longer to check if the network is actually overfitting and a simpler model is needed. . Lastly, there is no data augmentation even though it is almost de facto when training CNNs. It is possible to use image augmentations (flips, rotations, etc) even though they do not technically make sense on a spectrogram. It might be better to use augmentations directly inspired by signal processing like SpecAugment. By the way, SpecAugment is already included in fastaudio! Along with many other waveform and spectrogram augmentations. . To recap, there are many good reasons why normalization only helped a little on the ESC-50 dataset. The points above described some possible next steps to increase performance. . Conclusions . In this post we saw how spectrograms are fundamentally different from natural images. We then explored two ways of normalizing spectrograms when training neural networks: global normalization and channel-based normalization. . Next we implemented these two normalization techniques and tested them against an unnormalized baseline on the ESC-50 dataset. There was a small but noticeable gain. We examined why this gain was potentially so small, and offered some approaches that could boost performance. . In the end, the choice of spectrogram normalization likely depends on how the system will be used. For example, if the system will be deployed in an environment similar to the training environment, then normalizing by spectrogram channels makes more sense. This is because the training statistics will be a good match for the similar patterns and distributions in the deployed environment. However, it is critical to monitor the system in this environment and update the statistics as needed to avoid shifting out of domain. . If the system will instead be used in a completely different environment, of which you have no knowledge, then the global statistics might be a better fit. While not as technically sound, the model will (hopefully) be less surprised by radically new activity across the channels. . To recap, there is no one universally correct way to normalize spectrograms for every audio task. Like many aspects of deep learning, the final choice will be experimental and based on the specifics of both the problem and domain. . I hope this post gave you a good notion for how to normalize spectrograms. Even moreso, I hope that it gave you new ideas to try out. The ESC-50 is a great playground for new ideas, happy experimenting! .",
            "url": "https://enzokro.dev/normalizing_spectrums/2020/08/04/Normalizing-spectrograms-for-deep-learning.html",
            "relUrl": "/normalizing_spectrums/2020/08/04/Normalizing-spectrograms-for-deep-learning.html",
            "date": " • Aug 4, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Rayleigh Weight Initializations",
            "content": "Overview . This is the first in a series of posts that will build complex-valued neural networks in pytorch with an eye towards Radio Frequency (RF) applications. Part one introduces complex weight initializations using two pieces of information: . Magnitude: the length of a vector | Phase: where a vector is pointing | . The magnitudes will be drawn from a Rayleigh distribution which is a natural representation for RF data. Next, we make sure the weights are good initial values for neural networks by having their variance meet certain criteria. We then put everything together by creating complex weights as pytorch tensors. To start, we give some backgroud on weight initializations and why they are important. . Background on initializations . It is crucial to train neural networks from properly initialized weights. While initializations are now taken for granted, they were one of the key pieces that made it possible to train deep networks in the first place. The main insights about good weight initializations came from analyzing the variance of weights. Specifically, in how the weight&#39;s variance affected gradients during backpropagation. It was great work by He and Glorot, Bengio that showed how the variance of the weights should meet certain criteria to keep gradients flowing smoothly. Here, &quot;smoothly&quot; means that during training the gradients neither disappear (go to 0) nor explode (grow without bound). The best known initializations are now the defaults in popular deep learning libraries such as TensorFlow and pytorch. However, complex-valued deep networks are newer and not as well established. . Complex networks . Complex-valued networks have a long history, see Chapter 3 of this thesis for a great recap. The first modern and complete take on deep complex neural nets is likely Deep Complex Networks by Trabelsi et. al. This paper explored many fundamental building blocks for deep complex networks. It developed initializations, convolutions, activations, batch normalizations, and put them together into complex Residual Networks (ResNets). Despite this fantastic work the field stayed quiet at first. But, there has been a recent spike in activity with follow ups in: medical imaging, radio frequency (RF) signal processing, physical optical networks, and even some quantum networks! Next we take the first steps in making complex weights: creating their magnitudes. . Rayleigh Distribution . To best describe a Rayleigh distribution, imagine setting up a sensor that measures wind speed out in an open field. If we analyze the wind speed passing through this sensor in two directions, say North and East, then the magnitude of the wind&#39;s velocity will be Rayleigh distributed. A Rayleigh distribution happens in general when two random variables are added. To be Rayleigh distributed, the random variables must be uncorrelated, normally distributed, zero mean, and share the same standard deviation. A more relevant example for our purposes is tuning in to a radio signal. Imagine we tune a radio to an empty spectrum region where all we hear is noise. If we record the real and imaginary components of this RF stream, then their magnitudes will be Rayleigh distributed. In other words, the magnitude of RF noise follows a Rayleigh distribution. . Let&#39;s dive into the details. The equation below is the Probability Density Function (PDF) of a Rayleigh distribution. . $$f(x, sigma) = frac{x}{ sigma^2}e^{-x^2/(2 sigma^2)}, x geq 0$$ . If this equation looks intimidating, we can instead code it up as a python function to make it much clearer: . def rayleigh_pdf(x, sigma): &quot;Finds Rayleigh PDF evaluated at `x`.&quot; p = (x / sigma**2) * np.exp(-x**2 / (2*sigma**2)) return p . In the equation and code above sigma ($ sigma$) is the scale parameter. It is common in many probability distributions and usually controls how spread out or narrow a distribution is. . Let&#39;s start by setting $ sigma = 1$ to see the &quot;basic&quot; Rayleigh shape. We will then change sigma to see how it affects the distribution. . sigma = 1 # calculate PDF on 100 equally spaced points between 0 and 5 points = np.linspace(0, 5, 100) ray_pdf = rayleigh_pdf(points, sigma) . # plot the rayleigh pdf fig,ax = plt.subplots(figsize=(8,7)) ax.plot(ray_pdf) ax.set_xticklabels([-1] + list(range(len(ax.get_xticklabels())))) ax.set_xlabel(&#39;Sample Value&#39;, fontsize=&#39;x-large&#39;) ax.set_ylabel(&#39;Probability Density&#39;, fontsize=&#39;x-large&#39;) ax.set_title(&#39;Rayleigh PDF&#39;, fontsize=&#39;xx-large&#39;); . As we mentioned the scale, $ sigma$, changes the width or narrowness of the distribution. Let&#39;s both halve and double sigma to see what happens. . # setup plot fig,ax = plt.subplots(figsize=(8,7)) ax.set_xlabel(&#39;Sample Value&#39;, fontsize=&#39;x-large&#39;) ax.set_ylabel(&#39;Probability Density&#39;, fontsize=&#39;x-large&#39;) ax.set_title(&#39;Rayleigh PDFs&#39;, fontsize=&#39;xx-large&#39;); # plot sigmas in different colors sigmas = [0.5, 1, 2] colors = [&#39;m&#39;, &#39;b&#39;, &#39;r&#39;] for color,sig in zip(colors,sigmas): rpdf = rayleigh_pdf(points, sig) ax.plot(points, rpdf, c=color, label=f&#39;σ: {sig}&#39;) ax.set_xticklabels([-1] + list(range(len(ax.get_xticklabels())))) ax.legend(); . The blue line in plot above is the same PDF from the first plot where $ sigma = 1$. We can see how $ sigma = 0.5$ pulls the distribution up and to the left, while $ sigma = 2$ squishes it down and to the right. In other words, a smaller sigma makes our distribution narrower while a larger sigma makes it wider. . Plotting the theoretical Rayleigh PDF above only shows what the distribution should looks like. Now we need to actually generate the Rayleigh values. . Generating Rayleigh samples . We will use the RandomState class in the numpy library to generate Rayleigh samples. RandomState is a helpful class that can sample from just about every known distribution. . from numpy.random import RandomState . First we create the RandomState class with the chosen seed of $0$. . seed = 0 rand = RandomState(seed) . This class can directly sample from a Rayleigh distribution. We use the sampling function RandomState.rayleigh which accepts two parameters: . scale: $ sigma$ with a default value of 1. | size: the shape of the output array | . Let&#39;s start by drawing 1,000 Rayleigh samples with $ sigma = 1$. . sigma = 1 shape = 1000 # one dimensional vector with 1000 samples . ray_vals = rand.rayleigh(scale=sigma, size=shape) . How to check if these samples are actually Rayleigh distributed? We can refer back to our PDF plots at the beginning, which tell us how Rayleigh samples should be &quot;spread out&quot;. The easiest way to check if these samples are spread out as expected is with a histogram. . # plot histogram of 1000 drawn samples plt.figure(figsize=(8,7)) plt.hist(ray_vals) plt.xlabel(&#39;Sample Value&#39;, fontsize=&#39;x-large&#39;) plt.ylabel(&#39;Counts&#39;, fontsize=&#39;x-large&#39;) plt.title(f&#39;Histogram of {shape:,} Rayleigh samples&#39;, fontsize=&#39;xx-large&#39;); . That looks pretty good! As we generate more samples it should get even closer to the earlier PDF plots. Let&#39;s make sure this happens by now drawing 10,000 samples and using more bins. . large_shape = 10000 many_ray_vals = rand.rayleigh(scale=sigma, size=large_shape) plt.figure(figsize=(8,7)) plt.hist(many_ray_vals, bins=20); plt.xlabel(&#39;Sample Value&#39;, fontsize=&#39;x-large&#39;) plt.ylabel(&#39;Counts&#39;, fontsize=&#39;x-large&#39;) plt.title(f&#39;Histogram of {large_shape:,} Rayleigh samples&#39;, fontsize=&#39;xx-large&#39;); . Looking better. Let&#39;s compare this histogram against the theoretical Rayleigh PDF. Note that we pass density=True to the histogram function below to make it an approximate PDF. . # compare theoretical vs. sampled Rayleigh PDFs plt.figure(figsize=(8,7)) plt.hist(many_ray_vals, density=True, bins=20) # makes the histogram sum to one, to mimic pdf plt.plot(points, ray_pdf, c=&#39;r&#39;, label=&#39;theoretical rayleigh pdf&#39;, linewidth=3) plt.title(&#39;Sampled vs. Theoretical Rayleigh PDFs&#39;, fontsize=&#39;x-large&#39;) plt.xlabel(&#39;Sample Value&#39;, fontsize=&#39;x-large&#39;) plt.ylabel(&#39;Probability Density&#39;, fontsize=&#39;x-large&#39;) plt.legend(); . Adding phase information . With the Rayleigh values from above we now have the magnitude, or length, of the complex weights. But that is only one part of a complex number. We are still missing the phase, or angle, information. The phase tells us which direction in the complex plane a vector is pointing in. . For our purposes it is enough to use random angles. Why? Many processes such as speech, images, and RF modulations encode information in phase. We do not want to bias the networks to any of these particular phase setups. By adding random uniform phase, it is like starting with many vectors pointing out in all directions. Then, during training, the network learns how to best orient and scale the weights for its task. . Adding this random uniform phase is straightforward. We pick uniform samples from $- pi$ to $ pi$ radians which maps to a full loop of the unit circle. This can be done with the same RandomState class from before. . # pick random directions along the unit circle phase = rand.uniform(low=-np.pi, high=np.pi, size=ray_vals.shape) . The phase can split the Rayleigh magnitudes into real and imaginary components. We use the cosine of the phase for the real part, and the sine of the phase for the imaginary part. . real = ray_vals * np.cos(phase) imag = ray_vals * np.sin(phase) . Let&#39;s plot some of these values in the complex plane to see if we truly have randomly oriented vectors. We pick 100 random weights and expect to see vectors with different magnitudes spread out in all directions. . # plot a few complex weights chosen_samples = range(100) # plot 100 random samples plt.figure(figsize=(8,7)) for idx in chosen_samples: # index into phase and magnitude variables angle,mag = phase[idx],ray_vals[idx] plt.polar([0,angle],[0,mag], marker=&#39;o&#39;) # plot them starting from the origin . Matching He and Glorot variance criteria . Even though we have real and imaginary components, they are not good initializations yet. The polar plot above gives some clues as to why (hint: look at the range of magnitudes in the vectors). Recall from the background on initializations: the key insight was that the variance of the weights should meet certain criteria. Matching this criteria helps the gradients flow well during backpropagation. . To get more specific, both the He and Glorot criteria are based on the incoming and outgoing connections in a network layer. The number of connections are typically called fan_in and fan_out, respectively. . The He criteria says that the variance of weights $W$ should be: $$ text{Var}(W) = frac{2}{ text{fan_in}}$$ . The Glorot criteria says that the variance should be: $$ text{Var}(W) = frac{2}{ text{fan_in + fan_out}}$$ . Deep networks typically have hundreds or thousands of connections. In practice this means that the variance of the weights has to be very small. Now we can see why the values in the earlier polar plot are not good: their variance is clearly too large. . How can we generate Rayleigh samples that meet the He or Glorot variance criteria? The complex nets paper from earlier includes a nice derivation for the variance of a complex Rayleigh distribution: $$ text{Var}(W) = 2 sigma^{2}$$ We can set the Rayleigh variance equal to the He and Glorot criteria and solve for sigma. . To meet the He criteria, sigma should be: $$ sigma_{ text{He}} = frac{1}{ sqrt{ text{fan_in}}}$$ . To meet the Glorot criteria, sigma should be: $$ sigma_{ text{Glorot}} = frac{1}{ sqrt{ text{fan_in + fan_out}}}$$ . Let&#39;s take a step back. In the earlier sections we used a flat vector of complex weights as an example. It&#39;s as if we took a single series of wind velocity or RF sample measurements. Since the He and Glorot criteria are defined specifically for network layers, we now switch to a simple one-layer network as an example. We aribtrarily choose a layer with 100 inputs and 50 outputs (fan_in = 100, fan_out = 50). . Plugging in these fan_in and fan_out numbers into the sigma equations gives: $$ sigma_{ text{He}} = frac{1}{10}$$ . $$ sigma_{ text{Glorot}} = frac{1}{5 sqrt{6}}$$ . By plugging in these sigmas into the random sampler, it will draw Rayleigh values that match the chosen variance criteria. We can then add phase information in the same way as before. . Putting it all together: complex pytorch initializer . Here is a short recap of the previous sections: . Drew a flat series of Rayleigh magnitudes. | Picked a random phase component, then split the magnitudes into real and imaginary parts. | Saw how to match the He and Glorot criteria with Rayleigh samples for a single network layer. | To make the above practical and usable, we need to automatically generate complex weigths that: . Match the He/Glorot variance criteria | Are pytorch tensors | Have the correct shape for the given network layer | . We can do this in a python function. . One quick word about calculating fan_in and fan_out values. We saw the feed-forward case with our single network layer. There the number of incoming connections was simply fan_in and the outgoing connections were fan_out. However, the convolutional case is slightly more complicated. A convolutional layer has input and output feature maps which are roughly analogous to input and output units in feed-forward layers. But we also have to factor in the kernel size. Pytorch has a nice convenience function to help. . We can now package all of the code above, with some refactoring, into a function that automatically generates complex Rayleigh weights given an input pytorch module. . import torch def get_complex_inits(module, seed=None, criterion=&#39;he&#39;): &quot;Creates real and imaginary Rayleigh weights for initialization.&quot; # create random number generator rand = RandomState(seed if seed is None else torch.initial_seed()) # get shape of the weights weight_size = module.weight.size() # find number of input and output connection fan_in,fan_out = torch.nn.init._calculate_fan_in_and_fan_out(module.weight) # find sigma to meet chosen variance criteria assert criterion in (&#39;he&#39;,&#39;glorot&#39;) factor = fan_in if criterion == &#39;he&#39; else fan_in + fan_out sigma = 1. / np.sqrt(factor) # draw rayleigh magnitude samples magnitude = rand.rayleigh(scale=sigma, size=weight_size) # draw uniform angle samples phase = rand.uniform(low=-np.pi, high=np.pi, size=magnitude.shape) # split magnitudes into real and imaginary components real = magnitude * np.cos(phase) imag = magnitude * np.sin(phase) # turn into float tensors and return real,imag = map(torch.from_numpy, [real,imag]) return real,imag . Testing on a nn.Linear module . We can test this on a single layer using pytorch&#39;s nn.Linear module. . m = torch.nn.Linear(100, 50) real,imag = get_complex_inits(m) . Let&#39;s check if the weights are correctly distributed. Going back to our Rayleigh introduction, it is the magnitude that should be Rayleigh distributed. . # grab magnitude as flat vector of numpy samples magnitude = torch.sqrt(real**2 + imag**2).numpy().reshape(-1) . # pick points that cover sample range to compare with theoretical rayleigh pdf points = np.linspace(0, magnitude.max(), 1000) ray_pdf = rayleigh_pdf(points, sigma=1./np.sqrt(100)) # plot histogram of magnitudes vs. theoretical pdf plt.figure(figsize=(8,7)) plt.title(&#39;nn.Linear vs. Theoretical Rayleigh PDFs&#39;, fontsize=&#39;x-large&#39;) plt.xlabel(&#39;Sample Value&#39;, fontsize=&#39;x-large&#39;) plt.ylabel(&#39;Probability Density&#39;, fontsize=&#39;x-large&#39;) plt.hist(magnitude, density=True) plt.plot(points, ray_pdf, c=&#39;r&#39;, linewidth=3); . Success! . Testing on a nn.Conv2d module . What about a convolutional layer? Our main concern here is that both the tensor shape and fan_in/fan_out are handled correctly. . # make conv layer with 100 input features, 50 output features, and (3x3) kernel m = torch.nn.Conv2d(100, 50, 3) real,imag = get_complex_inits(m) # get the initial complex weights # make sure the shape of weights is ok print(f&#39;Shapes of real and imaginary convolution tensors: {real.shape}, {imag.shape}&#39;) . Shapes of real and imaginary convolution tensors: torch.Size([50, 100, 3, 3]), torch.Size([50, 100, 3, 3]) . Let&#39;s check if they are still Rayleigh distributed in the same way as before. . # grab magnitude as flat vector of numpy samples magnitude = torch.sqrt(real**2 + imag**2).numpy().reshape(-1) . # pick points that cover sample range to compare with theoretical rayleigh pdf points = np.linspace(0, magnitude.max(), 1000) # we need fan_in for the more complicated conv case fan_in,fan_out = torch.nn.init._calculate_fan_in_and_fan_out(m.weight) ray_pdf = rayleigh_pdf(points, sigma=1./np.sqrt(fan_in)) # plot histogram of magnitudes vs. theoretical pdf plt.figure(figsize=(8,7)) plt.title(&#39;nn.Conv2d vs. Theoretical Rayleigh PDFs&#39;, fontsize=&#39;x-large&#39;) plt.xlabel(&#39;Sample Value&#39;, fontsize=&#39;x-large&#39;) plt.ylabel(&#39;Probability Density&#39;, fontsize=&#39;x-large&#39;) plt.hist(magnitude, density=True) plt.plot(points, ray_pdf, c=&#39;r&#39;, linewidth=3); . Conclusion . In this post we created Rayleigh initializations for complex-valued neural networks. We started with an overview of the Rayleigh distribution. We then used this distribution to generate the magnitudes of complex weights. Next we added some phase information to randomly orient the vectors. After that, we made sure the weights matched a certain variance criteria to be good initializations. Lastly we put all that work together into a python function that returns pytorch tensors. The function as is not quite ready for use in a fully fledged complex-valued setup, but we will get there. . Part two will look at another type of complex initialization based on (semi) unitary matrixes. After that we will proceed to build complex convolutions and actvations. .",
            "url": "https://enzokro.dev/complex_networks/2020/07/20/Rayliegh-initialization-for-complex-networks.html",
            "relUrl": "/complex_networks/2020/07/20/Rayliegh-initialization-for-complex-networks.html",
            "date": " • Jul 20, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About",
          "content": "Researcher from Ecuador, working on RF applications with ML, speech, and video processing. .",
          "url": "https://enzokro.dev/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://enzokro.dev/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}