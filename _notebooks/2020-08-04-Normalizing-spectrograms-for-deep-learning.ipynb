{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# How to normalize spectrograms\n",
    "> Scaling spectrograms for neural networks.  \n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [normalizing_spectrums]\n",
    "- image: images/violin_spec.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction  \n",
    "\n",
    "Spectrograms are often treated as images in deep learning to leverage the many great techniques from image classification. A spectrogram, however, is fundamentally different from natural images in several ways as we will see below. That raises the central question of this post: what is the proper way to normalize spectrograms when training neural networks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Turning audio into an image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is hard to overstate the success of deep neural networks for classifying images. This challenging task was previously dominated by expert handcrafted features. Now, the features are automatically learned from labeled data instead. The performance of these learned features completely shifted the Computer Vision paradigm. We would ideally like to follow these same, proven approaches in audio tasks.\n",
    "\n",
    "However, audio is usually treated as a one dimensional signal in Machine Learning applications. Even stereo recordings with more than one channel are first mixed into mono (single channel) before processing. That means raw audio is unusable with 2D CNNs which are the bread and butter of modern image recognition. If we could represent audio in two dimensions, like an image, it opens up a host of deep learning image classification techniques.\n",
    "\n",
    "Thankfully there are many ways to transform audio into two dimensions. The most common one is the short-time Fourier Transform [(STFT)](https://www.dsprelated.com/freebooks/sasp/Short_Time_Fourier_Transform.html). The STFT turns audio into a spectrogram: a 2-D signal representation in time and frequency. Since a spectrogram is two dimensional we can treat it like an image!  \n",
    "\n",
    "Before plugging spectrograms into a neural network we need a data pipeline. Data normalization is one of the first steps in any good data pipeline. This is because learning is difficult with unnormalized inputs and convergence could take a very long time. For natural images, normalization involves: \n",
    "- Centering the image values by subtracting a single, scalar mean. \n",
    "- Dividing the images by a scalar standard deviation to give them a variance of 1.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample audio dataset\n",
    "\n",
    "\n",
    "To keep things practical, we apply these normalization techniques to a [sound classification challenge](https://github.com/fastaudio/Audio-Competition) hosted by `fastaudio`. [`fastaudio`](https://github.com/fastaudio/fastaudio) is a community extension of the [`fastai`](https://github.com/fastai/fastai) library to make neural network audio tasks more accessible.  \n",
    "The challenge here is to classify sounds in the [ESC-50 dataset](https://github.com/karolpiczak/ESC-50).\n",
    "ESC-50 stands for \"Environment Sound Classification with 50 classes\". This set has a diverse set of sounds which gives a feel for how different audio spectrograms can be. Every file is five seconds long which makes batch processing easier since the samples are of the same size.\n",
    "\n",
    "Many lines below are based on the `fastaudio` [baseline results notebook](https://github.com/fastaudio/Audio-Competition/blob/master/ESC-50-baseline-1Fold.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing `fastai` and `fastaudio` modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *\n",
    "from fastaudio.core.all import *\n",
    "from fastaudio.augment.all import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the ESC-50 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is downloading the ESC-50 dataset. It is included in `fastaudio` so we grab it with fastai's `untar_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# already in fastaudio, can download with fastai's `untar_data`\n",
    "path = untar_data(URLs.ESC50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking inside of the downloaded `audio` folder reveals many `.wav` files.  \n",
    "Below we view the files with the `ls` method, a handy fastai addition to python's standard `pathlib.Path` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wavs = (path/\"audio\").ls()\n",
    "wavs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of `ls` shows there are 2,000 audio files. But the filenames are not too descriptive, so how can we know what is actually in each one?  \n",
    "As with many datasets, the download includes a table with more information about the data (aka metadata)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the audio metadata and show its first few rows\n",
    "df = pd.read_csv(path/\"meta\"/\"esc50.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key information from the table is in the `filename` and `category` columns. The `filename` gives the name of the file inside of the `audio` folder. The `category` tells us which class it belongs to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pick the last file in the data directory as our working example. The file's `name` can index into the metadata table above to display its specific information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick the row where \"filename\" matches the waveform's \"name\".\n",
    "df.loc[df.filename == wavs[-1].name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a recording of crickets! We can load this audio file using the `create` function of the `AudioTensor` class in `fastaudio`. `AudioTensor` wraps a `torch.Tensor` with some extra syntactic sugar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an AudioTensor from a file path\n",
    "sample = AudioTensor.create(wavs[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks to `AudioTensor` we can directly plot and even listen to our sample. Each \"burst\" in the file is a cricket chirp. There are three full chirps and the early starts of a fourth chirp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizing waveforms\n",
    "\n",
    "The first step is normalizing the audio waveform itself. We give it a mean of zero and unit variance in the typical way: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{norm_audio} = \\frac{\\text{audio} - mean(\\text{audio})}{std(\\text{audio})} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the waveform\n",
    "norm_sample = (sample - sample.mean()) / sample.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if the mean is roughly 0 and the variance is roughly one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking if normalization worked\n",
    "norm_sample.mean(),norm_sample.var()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Success! The waveform is normalized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting spectrograms from audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to extract a spectrogram from the normalized audio. The `fastaudio` library wraps parts of [`torchaudio`](https://pytorch.org/audio/) to convert an `AudioTensor` into a spectrogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a fastaudio Transform that converts audio into spectrograms\n",
    "cfg = AudioConfig.BasicSpectrogram()\n",
    "audio2spec = AudioToSpec.from_cfg(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The spectrogram computation details are not important here. But a quick look at the [spectrogram source code](https://pytorch.org/audio/_modules/torchaudio/functional.html#spectrogram) shows that it boils down to some pre and post processing around a [torch.stft](https://pytorch.org/docs/stable/generated/torch.stft.html) call. We can now transform our audio into a spectrogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the spectrogram\n",
    "spec = audio2spec(norm_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like with the waveform, `fastaudio` can directly plot spectrograms. The colorbar on the right is especially helpful here, since `matplotlib` always normalizes the values in a plot to a certain color range. Withtout the colorbar, the fixed color range makes it impossible to know or even guess the exact values in a spectrogram plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a good time to compare the shapes of the audio vs. the spectrogram to see the added dimension that makes the spectrogram an \"image\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Audio shape [channels, samples]: {norm_sample.shape}')\n",
    "print(f'Spectrum shape [channels, bins, time_steps]: {spec.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How do we normalize spectrograms?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As stated in the introduction, a spectrogram is fundamentally different from an image.\n",
    "\n",
    "Both dimensions in an image are in the spatial domain and have the same units.\n",
    "Images are stored as integers in the range of `[0, 255]`.  \n",
    "To normalize first divide all pixels by 255, the max possible value, to map them into `[0, 1]`.  \n",
    "Then, find the statistics that approximately center the data and give it unit variance.  \n",
    "For a color image we have three channels (RGB) and normalize each one.\n",
    "If the image is grayscale then we normalize its single channel instead.\n",
    "Given the layout of natural images, and the fact that both dimensions are in the same domain, it makes sense to normalize each channel with single, global values.\n",
    "\n",
    "Spectrograms are different.\n",
    "In a spectrogram, one dimension represents time and the other represents frequency.\n",
    "Different quantities, scales, and sizes.\n",
    "Frequency dimension given by choice of FFT size. Sets our spectral resolution.\n",
    "Time dimension given by length of our signal, FFT size, and window overlap. Sets our temporal resolution.\n",
    "In fact what we call the spectrogram is actually the log of the power spectrum.\n",
    "Below we give a quick recap of how the spectrogram is computed to show how it differs from images.\n",
    "\n",
    "If $\\text{x}$ is the input audio then the STFT returns the spectrum:\n",
    "$$\\text{spectrum} = \\text{STFT(x)}$$\n",
    "We are more interested in the energy or power of the signal, so we take the absolute value of the STFT and square it:  \n",
    "$$\\text{power_spectrum} = |\\text{STFT(x)}|^2$$ \n",
    "While we could use the power spectrum as our input \"image\", it is a bit problematic. The power spectrum often has a few, strong peaks and many small values. This means the values are [heavy-tailed](https://danielsdiscoveries.wordpress.com/2017/09/29/spectrogram-input-normalisation-for-neural-networks/) and make for poor network inputs.  \n",
    "To deal with this we take the log of the power spectrum which spreads out the values. This becomes the spectrogram:\n",
    "$$\\text{spectrogram} = log(|\\text{STFT(x)}|^2)$$\n",
    "The range of the log function is from $-\\infty$ to $+\\infty$ which is very different than the integers from 0 to 255.\n",
    "\n",
    "\n",
    "However, the spectrogram also introduces the notion of a different type of channel.\n",
    "This makes \"channel\" an overloaded term for our purposes but it is still a crucial piece of the puzzle. \n",
    "The spectrogram transform can be interpreted as a \"channelizer\".\n",
    "That is a fancy way to say that it takes the continuous frequency spectrum of our signal and chops it up into discrete bins, or channels. For example, consider a signal sampled at 16 kHz (typical for audio) where we take an STFT of size 512. Our spectrogram will have 512 channels where each one has a \"bandwidth\" of $$16 \\ \\text{kHz} \\ \\ / \\ \\ 512 \\ \\text{bins} = 31.25 \\ \\text{Hz per bin}$$\n",
    "\n",
    "Even though these spectrum channels are different from the channels in an image, it raises the question: should we (or can we?) normalize an entire spectrum \"image\" with a single, global value? Or do we need to normalize each channel, as is done with images?  \n",
    "\n",
    "There is no clear answer here, and your approach will likely depend both on the specifics of your problem and where your system will be deployed.\n",
    "For example, if your are building a system that will be deployed in a similar environment as the training one, then it might make more sense to normalize by channels.\n",
    "Your channel-based normalization statistics will follow the average noise floor and activity of the training data.\n",
    "This motivation hold if you expect roughly the same patterns and distributions of activity once the system is deployed.\n",
    "However, it will be critical to monitor the deployed environment and update the statistics as needed, else you slowly shift out of domain.\n",
    "\n",
    "If your system will instead be used in a completely different environment, of which you have no knowledge, then the global statistics could be a better fit. While not as technically sound, your model won't be as surprised by radically new activity across different channels. \n",
    "\n",
    "Lastly, we also have issue of Transfer Learning. In Transfer Learning it is best-practice to normalize the new dataset with the statistics from the old dataset. In most cases that means normalizing with ImageNet statistics.\n",
    "So if you are doing transfer learning, the easiest approach will be to use original stats. \n",
    "If your dataset is large enough that you are training from scratch, then the above applies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spectrogram Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization statistics always come from the training set (this is a crucial place to avoid data leakage from the validation set). That means stepping through the training set once and getting a mean and standard deviation from each mini-batch. Then, we average the statistics from each mini-batch to get a pair of \"global\" statistics.  \n",
    "One small detail: if your training dataset is large enough, you often do not need to iterate through the entire set. It is enough to sample 10% to 20% of the dataset for accurate statistics. However, since ESC-50 is small enough we get statistics from the whole set. \n",
    "\n",
    "First we to accumulate these statistics over mini-batches. We can borrow and slightly refactor a class from the incredibly helpful guide [here](http://notmatthancock.github.io/2017/03/23/simple-batch-stat-updates.html).  \n",
    "The `StatsRecorder` class below tracks the mean and standard deviation across mini-batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StatsRecorder:\n",
    "    def __init__(self, red_dims=(0,2,3)):\n",
    "        \"\"\"Accumulates normalization statistics across mini-batches.\n",
    "        ref: http://notmatthancock.github.io/2017/03/23/simple-batch-stat-updates.html\n",
    "        \"\"\"\n",
    "        self.red_dims = red_dims # which mini-batch dimensions to average over\n",
    "        self.nobservations = 0   # running number of observations\n",
    "\n",
    "    def update(self, data):\n",
    "        \"\"\"\n",
    "        data: ndarray, shape (nobservations, ndimensions)\n",
    "        \"\"\"\n",
    "        # initialize stats and dimensions on first batch\n",
    "        if self.nobservations == 0:\n",
    "            self.mean = data.mean(dim=self.red_dims, keepdim=True)\n",
    "            self.std  = data.std (dim=self.red_dims,keepdim=True)\n",
    "            self.nobservations = data.shape[0]\n",
    "            self.ndimensions   = data.shape[1]\n",
    "        else:\n",
    "            if data.shape[1] != self.ndimensions:\n",
    "                raise ValueError('Data dims do not match previous observations.')\n",
    "            \n",
    "            # find mean of new mini batch\n",
    "            newmean = data.mean(dim=self.red_dims, keepdim=True)\n",
    "            newstd  = data.std(dim=self.red_dims, keepdim=True)\n",
    "            \n",
    "            # update number of observations\n",
    "            m = self.nobservations * 1.0\n",
    "            n = data.shape[0]\n",
    "\n",
    "            # update running statistics\n",
    "            tmp = self.mean\n",
    "            self.mean = m/(m+n)*tmp + n/(m+n)*newmean\n",
    "            self.std  = m/(m+n)*self.std**2 + n/(m+n)*newstd**2 +\\\n",
    "                        m*n/(m+n)**2 * (tmp - newmean)**2\n",
    "            self.std  = torch.sqrt(self.std)\n",
    "                                 \n",
    "            # update total number of seen samples\n",
    "            self.nobservations += n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default `StatsRecorder` averages over the image channel dimensions (grayscale or RGB). The `red_dims` argument might look familiar from normalization code in other Computer Vision tasks (even the `Normalize` in `fastai`).  \n",
    "Averaging instead over spectrogram channels is as easy as passing a different `red_dims`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the dataset loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to step through the training dataset. The setup belows follows the `fastaudio` ESC-50 baseline. One thing to mention is that by default, `fastaudio` resamples audio to 16 kHz. While much of the audio in the wild is sampled at 16 kHz, ESC-50 is actually sampled at a higher 44.1 kHz rate. Downsampling risks throwing away some information. But, keeping the higher sampling rate almost triples the \"width\" of the spectrogram. This much larger image potentially limits our batch size and architecture choices. For now we stick with downsampling to 16 kHz since it yields a very reasonable spectrogram shape of [201, 401]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need a `Transform` that normalizes individual audio waveforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioNormalize(Transform):\n",
    "    \"Normalizes a single `AudioTensor`.\"\n",
    "    def encodes(self, x:AudioTensor): return (x-x.mean()) / x.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CrossValidationSplitter(col='fold', fold=1):\n",
    "    \"Split `items` (supposed to be a dataframe) by fold in `col`\"\n",
    "    def _inner(o):\n",
    "        assert isinstance(o, pd.DataFrame), \"ColSplitter only works when your items are a pandas DataFrame\"\n",
    "        col_values = o.iloc[:,col] if isinstance(col, int) else o[col]\n",
    "        valid_idx = (col_values == fold).values.astype('bool')\n",
    "        return IndexSplitter(mask2idxs(valid_idx))(o)\n",
    "    return _inner\n",
    "\n",
    "# do not resample audio\n",
    "auds = DataBlock(blocks=(AudioBlock, CategoryBlock),  \n",
    "                 get_x=ColReader(\"filename\", pref=path/\"audio\"), \n",
    "                 splitter=CrossValidationSplitter(fold=1),\n",
    "                 item_tfms = [AudioNormalize],\n",
    "                 batch_tfms = [audio2spec],\n",
    "                 get_y=ColReader(\"category\"))\n",
    "dbunch = auds.dataloaders(df, bs=64)\n",
    "dbunch.show_batch(figsize=(7,7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding spectrogram normalization statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we create two statistic recorders: one for global statistics and one for channel-based statistics. Then we step through the entire dataset and find the normalization stats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create recorders\n",
    "global_stats  = StatsRecorder()\n",
    "channel_stats = StatsRecorder(red_dims=(0,1,3))\n",
    "\n",
    "# step through the dataset\n",
    "with torch.no_grad():\n",
    "    for idx,(x,y) in enumerate(iter(dbunch.train)):\n",
    "        # update normalization statistics\n",
    "        global_stats.update(x)\n",
    "        channel_stats.update(x)\n",
    "    \n",
    "# parse out the stats\n",
    "global_mean,global_std = global_stats.mean,global_stats.std\n",
    "channel_mean,channel_std = channel_stats.mean,channel_stats.std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the shape of both statistics to make sure they are as expected. For the global \"grayscale\" statistics, we expect a shape of: `[1,1,1,1]`. With spectrogram channel normalizations, we expect a shape of `[1,1,201,1]` with one normalization stat for each spectrogram bin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Shapes of global mean/std:')\n",
    "global_mean.shape,global_std.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Shapes of channel mean/std:')\n",
    "channel_mean.shape,channel_mean.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Normalization `Transforms`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to extend the `Normalize` in `fastai` to normalize the spectrogram mini-batches. The reason is type dispatch. `fastai` normalization uses ImageNet statistics due to the focus on transfer learning with color images. But this ImageNet normalization is only defined for the `TensorImage` class, while `AudioSpectrogram` subclasses the different `TensorImageBase`. The solution is to define `encodes` and `decodes` for `TensorImageBase` instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpecNormalize(Normalize):\n",
    "    \"Normalize/denorm batch of `TensorImage`\"\n",
    "    def encodes(self, x:TensorImageBase): return (x-self.mean) / self.std\n",
    "    def decodes(self, x:TensorImageBase):\n",
    "        f = to_cpu if x.device.type=='cpu' else noop\n",
    "        return (x*f(self.std) + f(self.mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make global and channel normalizers\n",
    "GlobalSpecNorm  = SpecNormalize(global_mean,  global_std,  axes=(0,2,3))\n",
    "ChannelSpecNorm = SpecNormalize(channel_mean, channel_std, axes=(0,1,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training with different statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the moment of truth. We train with the two different spectrogram normalizations and measure their impact. For this we again follow the `fastaudio` baseline and train each type of normalization for 20 epochs. The final score is the averaged accuracy of five runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "num_runs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding a good learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning rate is arguably the most critical neural network hyperparameter. The `lr_find` function in `fastai` is a great empirical way to set a good learning rate. The default learning rate in `cnn_learner` (0.001) is a good starting point. But since our task is so different from natural images it is worth re-evaluating this assumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auds = DataBlock(blocks=(AudioBlock, CategoryBlock),  \n",
    "#                  get_x=ColReader(\"filename\", pref=path/\"audio\"), \n",
    "#                  splitter=CrossValidationSplitter(fold=1), \n",
    "#                  item_tfms = [AudioNormalize], \n",
    "#                  batch_tfms = [audio2spec, GlobalSpecNorm],\n",
    "#                  get_y=ColReader(\"category\"))\n",
    "# dbunch = auds.dataloaders(df, bs=64)\n",
    "\n",
    "# learn = cnn_learner(dbunch, \n",
    "#                     xresnet18, \n",
    "#                     pretrained=False,\n",
    "#                     config=cnn_config(n_in=1),\n",
    "#                     loss_fn=CrossEntropyLossFlat,\n",
    "#                     metrics=[accuracy],)\n",
    "# learn.lr_find()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems we can make the learning rate higher than the default. 2e-3 looks like a good point in the curve, and we could potentially go even higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_lr = 2e-3 # from find_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training helpers  \n",
    "\n",
    "To avoid repeating ourselves, the helper functions below will build dataloaders and run the training loops.  \n",
    "The `get_dls` function makes it clear which normalization is being used. The `train_loops` function repeats training runs a given number of times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dls(bs=64, item_tfms=[], batch_tfms=[]):\n",
    "    \"Get dataloaders with given `bs` and batch/item tfms.\"\n",
    "    auds = DataBlock(blocks=(AudioBlock, CategoryBlock),  \n",
    "                     get_x=ColReader(\"filename\", pref=path/\"audio\"), \n",
    "                     splitter=CrossValidationSplitter(fold=1),\n",
    "                     item_tfms=item_tfms,   # for waveform normalization\n",
    "                     batch_tfms=batch_tfms, # for spectrogram normalization\n",
    "                     get_y=ColReader(\"category\"))\n",
    "    dls = auds.dataloaders(df, bs=bs)\n",
    "    return dls\n",
    "\n",
    "def train_loops(dls, name, num_runs=num_runs, epochs=epochs):\n",
    "    \"Runs `num_runs` training loops with `dls` for given `epochs`.\"\n",
    "    accuracies = []\n",
    "    for i in range(num_runs):\n",
    "        learn = cnn_learner(dls, \n",
    "                            xresnet18, \n",
    "                            pretrained=False,\n",
    "                            config=cnn_config(n_in=1),\n",
    "                            loss_fn=CrossEntropyLossFlat,\n",
    "                            metrics=[accuracy],)\n",
    "        learn.fit_one_cycle(epochs, good_lr)\n",
    "        accuracies.append(learn.recorder.values[-1][-1])\n",
    "    print(f'Average accuracy for \"{name}\": {sum(accuracies) / num_runs}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline performance  \n",
    "\n",
    "Before getting carried away with normalization, we have to first know where we stand. Setting a baseline without normalizations means we can later evaluate the impact of normalization. Else we cannot know if normalization helped at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # data without normalization\n",
    "# dls = get_dls(batch_tfms=[audio2spec])\n",
    "# # run training loops\n",
    "# train_loops(dls, name='No Norm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance with global statistics \n",
    "Next we normalize each audio waveform and the spectrograms with scalar global statistics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # data with waveform and global normalization\n",
    "# dls = get_dls(item_tfms=[AudioNormalize],\n",
    "#               batch_tfms=[audio2spec, GlobalSpecNorm])\n",
    "# # run training loops\n",
    "# train_loops(dls, name='Global Norm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance with channel statistics  \n",
    "Finally, we normalize each audio waveform and the spectrograms with channel-based statistics.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # get data with waveform and channel normalization\n",
    "# dls = get_dls(item_tfms=[AudioNormalize],\n",
    "#               batch_tfms=[audio2spec, ChannelSpecNorm])\n",
    "# # run training loops\n",
    "# train_loops(dls, name='Channel Norm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this post we explored some issues around normalizing spectrograms for neural network training. We saw how spectrograms are fundamentally different than natural images and how there are at least two ways of normalizing them. \n",
    "\n",
    "We then implemented these two normalization techniques and tested them against a baseline in the `fastaudio` ESC-50 competition. \n",
    "\n",
    "There was a noticeable gain from using the global type of normalization, and a moderate gain from using the channel-based normalization. This makes intuitive sense, since a simple `show_batch` showed how varied these spectrograms were. In other words, there is a large amount of intra and inter channel variability both within and across classes. Under these conditions we'd expect that a more general, global normalization better suits this task. If the samples all came from a consistent source, say speech spectrograms, then the per-channel normalization might fare better.  \n",
    "\n",
    "However, at the end of the day, there is no single theoretically correct way to normalize spectrograms for deep neural networks. Like many aspects of this field, the design choices will be experimental and depend on both the domain and problem specifics.  \n",
    "\n",
    "I hope this post gave you a good notion for how to normalize spectrograms. I also hope it gave you some ideas, and potential approaches to try yourself! The ESC-50 challenge is an excellent playground to try them out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For fun, going as high as we can based on ImageNette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from fastai.basics import *\n",
    "# from fastai.vision.all import *\n",
    "# from fastai.callback.all import *\n",
    "# from fastai.distributed import *\n",
    "# from fastprogress import fastprogress\n",
    "# from torchvision.models import *\n",
    "# from fastai.vision.models.xresnet import *\n",
    "# from fastai.callback.mixup import *\n",
    "# from fastscript import *\n",
    "\n",
    "# auds = DataBlock(blocks=(AudioBlock, CategoryBlock),  \n",
    "#                  get_x=ColReader(\"filename\", pref=path/\"audio\"), \n",
    "#                  splitter=CrossValidationSplitter(fold=1),\n",
    "#                  item_tfms = [AudioNormalize], \n",
    "#                  batch_tfms = [audio2spec, GlobalSpecNorm],\n",
    "#                  get_y=ColReader(\"category\"))\n",
    "# dbunch = auds.dataloaders(df, bs=64)\n",
    "\n",
    "# # with mixup, train for longer\n",
    "# epochs = 80\n",
    "\n",
    "# # ranger optimizer \n",
    "# lr      = 2e-3\n",
    "# mom     = 0.9\n",
    "# sqrmom  = 0.99\n",
    "# eps     = 1e-6\n",
    "# beta    = 0\n",
    "# opt_func = partial(ranger, mom=mom, sqr_mom=sqrmom, eps=eps, beta=beta)\n",
    "\n",
    "# # default pooling\n",
    "# pool = AvgPool\n",
    "\n",
    "# # mish activation\n",
    "# act_fn = Mish\n",
    "\n",
    "# # loss function\n",
    "# loss_func = LabelSmoothingCrossEntropyFlat()\n",
    "\n",
    "# # with mixup augmentation\n",
    "# mixup = True\n",
    "# alpha = 0.4\n",
    "# cbs = MixUp(alpha) if mixup else []\n",
    "\n",
    "# # whether to use self-attention\n",
    "# sa  = False\n",
    "# sym = False\n",
    "\n",
    "# # weight decay\n",
    "# wd = 1e-2\n",
    "\n",
    "# # the context manager way of dp/ddp, both can handle single GPU base case.\n",
    "# gpu = 0\n",
    "# n_gpu = torch.cuda.device_count()\n",
    "# # ctx = learn.parallel_ctx if gpu is None and n_gpu else learn.distrib_ctx\n",
    "# # ctx = learn.distrib_ctx\n",
    "\n",
    "# # # model\n",
    "# # n_out = 50\n",
    "# # m = xse_resnext18\n",
    "# # model = m(n_out=n_out, c_in=1, act_cls=act_fn, sa=sa, sym=sym, pool=pool)\n",
    "\n",
    "# accuracies = []\n",
    "# for run in range(num_runs):\n",
    "#     print(f'Run: {run}')\n",
    "# #     learn = Learner(dbunch, , opt_func=opt_func, \\\n",
    "# #             metrics=[accuracy], loss_func=loss_func)\n",
    "#     learn = cnn_learner(dbunch, \n",
    "#                         xse_resnext18,\n",
    "#                         pretrained=False,\n",
    "#                         config=cnn_config(n_in=1),\n",
    "#                         loss_fn=CrossEntropyLossFlat,\n",
    "#                         opt_func=opt_func,\n",
    "#                         metrics=[accuracy])\n",
    "#     ctx = learn.parallel_ctx if gpu is None and n_gpu else learn.distrib_ctx\n",
    "#     with partial(ctx, gpu)(): # distributed traing requires \"-m fastai.launch\"\n",
    "#         print(f\"Training in {ctx.__name__} context on GPU {gpu if gpu is not None else list(range(n_gpu))}\")\n",
    "#         learn.fit_flat_cos(epochs, lr, wd=wd, cbs=cbs)\n",
    "# #         learn.fine_tune(epochs, lr, wd=wd, cbs=cbs)\n",
    "#     accuracies.append(learn.recorder.values[-1][-1])\n",
    "\n",
    "# print(f'Average accuracy for ImageNet training: {sum(accuracies) / num_runs}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "336px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
