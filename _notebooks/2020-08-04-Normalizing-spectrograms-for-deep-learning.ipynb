{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to normalize spectrograms\n",
    "> Scaling spectrograms as inputs to neural networks.  \n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [normalizing_spectrums]\n",
    "- image: images/violin_spec.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Audio is naturally a 1D signal\n",
    "Can transformed into 2D in several ways\n",
    "Most common is via short-time Fourier Transform (STFT)\n",
    "The STFT turns audio into a spectrogram: 2D representation in time and frequency\n",
    "Since now 2D, can think of as an image\n",
    "Means we can leverage all the great work from deep learning image classification\n",
    "\n",
    "\n",
    "Hard to overstate success of deep neural network for image classification.\n",
    "Previously challenging task dominated by handcrafted features\n",
    "Now features automatically learned from labeled data\n",
    "Possible due to improvements in datasets, algorithms, and compute\n",
    "\n",
    "\n",
    "But, one tricky issue using spectrogram inputs\n",
    "Data must be properly normalized when training neural network\n",
    "Else learning is difficult for the networks and could take a very long time or even diverge.\n",
    "A spectrogram, however, is fundamentally different from natural images.\n",
    "So how can we properly normalize spectrograms?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background on initializations\n",
    "\n",
    "well known how to scale images: stores as 0 to 255 range. Then converted to range 0 to 1\n",
    "Find the statistics that approximately center the data and give it unit variance \n",
    "Spectrograms are created at a completely different range\n",
    "Usually analyzed in the log domain where the possible range is -inf to +inf. \n",
    "In practice the values more constrained, but range is much larger than images (and negative).\n",
    "\n",
    "Several details in how to normalize audio pop up.\n",
    "How to normalize the waveform?\n",
    "How to normalize the spectrogram?\n",
    "How do we compute the normalization statistics?\n",
    "How to deal with rapidly changing audio?\n",
    "\n",
    "Will start with normalizing waveforms then move to spectrograms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample audio dataset\n",
    "\n",
    "\n",
    "To make things practical, will apply normalization techniques to ongoing ESC-50 challenge hosted by fastai audio [fastaudio](https://github.com/fastaudio/fastaudio) library.\n",
    "Challenge uses the ESC-50 dataset for sound classification\n",
    "ESC-50 has a wide range of sounds, will give us a good feel for how varied spectrograms can be\n",
    "\n",
    "\n",
    "The fastaudio library is rapidly changing and improving so instructions might be different, but will aim to keep post updated.\n",
    "Many lines below are based on the [baseline results notebook](https://github.com/fastaudio/Audio-Competition/blob/master/ESC-50-baseline-1Fold.ipynb) for convenience. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing `fastai` and `fastaudio` modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *\n",
    "from fastaudio.core.all import *\n",
    "from fastaudio.augment.all import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the ESC-50 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# already included in fastaudio, can download with fastai's `untar_data`\n",
    "path = untar_data(URLs.ESC50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ESC-50 data is inside of an `audio` folder. \n",
    "Can look inside to find many `.wav` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wavs = (path/\"audio\").ls()\n",
    "wavs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the first waveform as an example for normalization. Can load an audio file using the `create` function of the `AudioTensor` class in `fastaudio`. This class wraps a `torch.Tensor` with added syntactic sugar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an AudioTensor from a file path\n",
    "sample = AudioTensor.create(wavs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks to functionality in the `AudioTensor` class we can easily plot and even listen to our sample!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.hear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by normalizing this waveform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizing waveforms\n",
    "\n",
    "The first step is normalizing the audio waveform.\n",
    "We give it a mean of zero and unit variance as in the usual way: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{norm_audio} = \\frac{\\text{audio} - mean(\\text{audio})}{std(\\text{audio})} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the waveform\n",
    "norm_sample = (sample - sample.mean()) / sample.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if the mean is roughly 0 and the variance is roughly one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking if normalization worked\n",
    "norm_sample.mean(),norm_sample.var()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Success! Let's wrap it back an `AudioTensor` for convenience. The sampling rate did not change so we pass in the sampling rate from the unnormalized waveform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_sample = AudioTensor(norm_saple, sr=sample.sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we normalized the audio we can convert it to a spectrogram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting spectrograms from audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now extract a spectrogram from the normalized audio.\n",
    "The fastaudio library has a helpful way of converting `AudioTensor`s into Spectrograms by wrapping some parts of [`torchaudio`](https://pytorch.org/audio/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a fastai Transform that converts audio into spectrograms\n",
    "cfg = AudioConfig.BasicSpectrogram()\n",
    "audio2spec = AudioToSpec.from_cfg(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The details of the spectrogram are not directly relevant here.\n",
    "But you can take a look at the [spectrogram source code](https://pytorch.org/audio/_modules/torchaudio/functional.html#spectrogram) to see it basically does pre and post processing around a [torch.stft](https://pytorch.org/docs/stable/generated/torch.stft.html) call.\n",
    "We can now transform our audio into a spectrogram and show it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract and view our spectrogram\n",
    "spec = audio2spec(norm_sample)\n",
    "spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a good time to compare the shapes of the audio vs. spectrogram to see how it went from one to two dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f'Audio shape: {norm_sample.shape} | Spectrogram shape: {spec.shape}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How do we normalize spectrograms?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As stated in the introduction, a spectrogram is fundamentally different from an image.\n",
    "\n",
    "\n",
    "In an image, both dimensions are in the spatial domain and have the same units.\n",
    "For a color image, we have three channels (RGB) and we normalize each one.\n",
    "If the image has a single channel (grayscale) then we normalize it instead.\n",
    "Given that both dimensions are in the same scale and domain same, and the general layout of natural images, it makes sense to normalize each channel with a single, global value.\n",
    "\n",
    "In a spectrogram, one dimension represents time and the other represents frequency.\n",
    "Different quantities, scales, and sizes.\n",
    "Frequency dimension given by choice of FFT size. Sets our spectral resolution.\n",
    "Time dimension given by length of our signal, FFT size, and window overlap. Sets our temporal resolution.\n",
    "\n",
    "However, the spectrogram also introduces the notion of a different type of channel.\n",
    "This makes \"channel\" an overloaded term for our purposes but it is still a crucial piece of the puzzle. \n",
    "The spectrogram transform can be interpreted as a \"channelizer\".\n",
    "That is a fancy way to say that it takes the continuous frequency spectrum of our signal and chops it up into discrete bins, or channels. For example, consider a signal sampled at 16 kHz (typical for audio) where we take an STFT of size 512. Our spectrogram will have 512 channels where each one has a \"bandwidth\" of $$16 \\ \\text{kHz} \\ \\ / \\ \\ 512 \\ \\text{bins} = 31.25 \\ \\text{Hz per bin}$$\n",
    "\n",
    "Even though these spectrum channels are different from the channels in an image, it raises the question: should we (or can we?) normalize an entire spectrum \"image\" with a single, global value? Or do we need to normalize each channel, as is done with images?  \n",
    "\n",
    "There is no clear answer here, and your approach will likely depend both on the specifics of your problem and where your system will be deployed.\n",
    "For example, if your are building a system that will be deployed in a similar environment as the training one, then it might make more sense to normalize by channels.\n",
    "Your channel-based normalization statistics will follow the average noise floor and activity of the training data.\n",
    "This motivation hold if you expect roughly the same patterns and distributions of activity once the system is deployed.\n",
    "However, it will be critical to monitor the deployed environment and update the statistics as needed, else you slowly shift out of domain.\n",
    "\n",
    "If your system will instead be used in a completely different environment, of which you have no knowledge, then the global statistics could be a better fit. While not as technically sound, your model won't be as surprised by radically new activity across different channels. \n",
    "\n",
    "Lastly, we also have issue of Transfer Learning. In Transfer Learning it is best-practice to normalize the new dataset with the statistics from the old dataset. In most cases that means normalizing with ImageNet statistics.\n",
    "So if you are doing transfer learning, the easiest approach will be to use original stats. \n",
    "If your dataset is large enough that you are training from scratch, then the above applies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Spectrogram Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by using a single, global value to normalize the spectrograms\n",
    "This is the same way images are normalized\n",
    "Will treat spectrogram as single-channel image\n",
    "Need to find a single mean and standard deviation to apply to to each image\n",
    "\n",
    "Get it from training dataset.\n",
    "This means stepping through our mini-batches and finding the mean and standard deviation for each batch.\n",
    "Then, accumulate and average it over our training samples to get a \"global\" statistic. \n",
    "First, we need a way to accumulate these statistics over mini-batches. Will borrow from the very helpful guide [here](http://notmatthancock.github.io/2017/03/23/simple-batch-stat-updates.html)\n",
    "\n",
    "One small detail: if your training dataset is large enough, you do not need to iterate through the entire thing.\n",
    "It is often enough to sample only 10 to 20% of the samples for accurate statistics.\n",
    "Since ESC-50 is small enough, we get statistics from the whole set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class below tracks our global mean and standard deviation across mini-batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StatsRecorder:\n",
    "    def __init__(self, red_dims=(0,2,3)):\n",
    "        \"\"\"Accumulates normalization statistics across mini-batches.\n",
    "        ref: http://notmatthancock.github.io/2017/03/23/simple-batch-stat-updates.html\n",
    "        \"\"\"\n",
    "        self.red_dims = red_dims # which mini-batch dimensions to average over\n",
    "        self.nobservations = 0   # running number of seen observations\n",
    "\n",
    "    def update(self, data):\n",
    "        \"\"\"\n",
    "        data: ndarray, shape (nobservations, ndimensions)\n",
    "        \"\"\"\n",
    "        # initialize stats and dimensions on first batch\n",
    "        if self.nobservations == 0:\n",
    "            self.mean = data.mean(dim=self.red_dims, keepdim=True)\n",
    "            self.std  = data.std (dim=self.red_dims,keepdim=True)\n",
    "            self.nobservations = data.shape[0]\n",
    "            self.ndimensions   = data.shape[1]\n",
    "        else:\n",
    "            if data.shape[1] != self.ndimensions:\n",
    "                raise ValueError('Data dims don't match prev observations.')\n",
    "            \n",
    "            # find mean of new mini batch\n",
    "            newmean = data.mean(dim=self.red_dims, keepdim=True)\n",
    "            newstd  = data.std(dim=self.red_dims, keepdim=True)\n",
    "            \n",
    "            # update number of observations\n",
    "            m = self.nobservations * 1.0\n",
    "            n = data.shape[0]\n",
    "\n",
    "            # update running statistics\n",
    "            tmp = self.mean\n",
    "            self.mean = m/(m+n)*tmp + n/(m+n)*newmean\n",
    "            self.std  = m/(m+n)*self.std**2 + n/(m+n)*newstd**2 +\\\n",
    "                        m*n/(m+n)**2 * (tmp - newmean)**2\n",
    "            self.std  = torch.sqrt(self.std)\n",
    "                                 \n",
    "            # update total number of seen samples\n",
    "            self.nobservations += n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "By default, it will average the statistics over grayscale or RGB dimensions for images. \n",
    "The red_dims might look familiar from other Computer Vision normalization code. \n",
    "Later on we will normalize by spectrogram channels by passing a different `red_dims`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get normalization stats from training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to iterate through our training dataset and find the global statistics. \n",
    "Setup follows the fastaudio ESC-50 baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path/\"meta\"/\"esc50.csv\")\n",
    "df.head()\n",
    "\n",
    "def CrossValidationSplitter(col='fold', fold=1):\n",
    "    \"Split `items` (supposed to be a dataframe) by fold in `col`\"\n",
    "    def _inner(o):\n",
    "        assert isinstance(o, pd.DataFrame), \"ColSplitter only works when your items are a pandas DataFrame\"\n",
    "        col_values = o.iloc[:,col] if isinstance(col, int) else o[col]\n",
    "        valid_idx = (col_values == fold).values.astype('bool')\n",
    "        return IndexSplitter(mask2idxs(valid_idx))(o)\n",
    "    return _inner\n",
    "\n",
    "auds = DataBlock(blocks=(AudioBlock, CategoryBlock),  \n",
    "                 get_x=ColReader(\"filename\", pref=path/\"audio\"), \n",
    "                 splitter=CrossValidationSplitter(fold=1),\n",
    "                 batch_tfms = [a2s],\n",
    "                 get_y=ColReader(\"category\"))\n",
    "dbunch = auds.dataloaders(df, bs=64)\n",
    "dbunch.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create our recorder and find the needed normalization statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_stats = StatsRecorder()\n",
    "for idx,o in enumerate(iter(dbunch.train)):\n",
    "    x,y = o\n",
    "    global_stats.update(x)\n",
    "global_mean,global_std = global_stats.mean,global_stats.std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can check they are the same shape as typical grayscale normalization stats. With a single channel, we expect shape: `[1,1,1,1]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean,mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std,std.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's repeat this with new `red_dims` argument to find normalization stats for each spectrogram channel. The new red_dims tells the recorder to average over everything except the frequency axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_stats = StatsRecorder(red_dims=(0,1,3))\n",
    "for idx,o in enumerate(iter(dbunch.train)):\n",
    "    x,y = o\n",
    "    channel_stats.update(x)\n",
    "channel_mean,channel_std = channel_stats.mean,channel_stats.std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Normalization transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First need a transform to normalize the audio as shown in the first section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioNormalize(Transform):\n",
    "    \"Normalizes a single audio tensor.\"\n",
    "    def encodes(self, x:AudioTensor): return (x-x.mean()) / x.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To normalize our spectrogram batches, we can reuse fastai's existing Normalize with different arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GlobalSpecNorm  = Normalize(global_mean,  global_std,  axes=(0,2,3))\n",
    "ChannelSpecNorm = Normalize(channel_mean, channel_std, axes=(0,1,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training with different statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follow the fastaudio baseline, and train each type of normalization for 10 epochs. \n",
    "Take averaged accuracy over five runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "num_runs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline performance  \n",
    "\n",
    "Before getting carried away with normalization, let's first find out where we stand.  \n",
    "Set baseline without normalization.  \n",
    "This is same loop as in baseline results notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auds = DataBlock(blocks=(AudioBlock, CategoryBlock),  \n",
    "                 get_x=ColReader(\"filename\", pref=path/\"audio\"), \n",
    "                 splitter=CrossValidationSplitter(fold=1), \n",
    "                 batch_tfms = [a2s],\n",
    "                 get_y=ColReader(\"category\"))\n",
    "dbunch = auds.dataloaders(df, bs=64)\n",
    "\n",
    "accuracies = []\n",
    "for i in range(num_runs):\n",
    "    learn = cnn_learner(dbunch, \n",
    "                    resnet18, \n",
    "                    normalize=False,\n",
    "                    config=cnn_config(n_in=1),\n",
    "                    loss_fn=CrossEntropyLossFlat,\n",
    "                    metrics=[accuracy]                 \n",
    "                    )\n",
    "    learn.fit_one_cycle(epochs)\n",
    "    accuracies.append(learn.recorder.values[-1][-1])\n",
    "\n",
    "print(f'Average accuracy without any normalization: {sum(accuracies) / num_runs}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance with global statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auds = DataBlock(blocks=(AudioBlock, CategoryBlock),  \n",
    "                 get_x=ColReader(\"filename\", pref=path/\"audio\"), \n",
    "                 splitter=CrossValidationSplitter(fold=1),\n",
    "                 item_tfms = [AudioNormalize], \n",
    "                 batch_tfms = [a2s, GlobalSpecNorm],\n",
    "                 get_y=ColReader(\"category\"))\n",
    "dbunch = auds.dataloaders(df, bs=64)\n",
    "\n",
    "\n",
    "accuracies = []\n",
    "for i in range(num_runs):\n",
    "    # make cnn learner\n",
    "    learn = cnn_learner(dbunch, \n",
    "                        resnet18, \n",
    "                        config=cnn_config(n_in=1),\n",
    "                        loss_fn=CrossEntropyLossFlat,\n",
    "                        metrics=[accuracy])\n",
    "    # fit one cycle for given epochs\n",
    "    learn.fit_one_cycle(epochs)\n",
    "    accuracies.append(learn.recorder.values[-1][-1])\n",
    "\n",
    "print(sum(accuracies) / num_runs)\n",
    "\n",
    "print(f'Average accuracy for \"global\" normalization: {sum(accuracies) / num_runs}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance with channel statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auds = DataBlock(blocks=(AudioBlock, CategoryBlock),  \n",
    "                 get_x=ColReader(\"filename\", pref=path/\"audio\"), \n",
    "                 splitter=CrossValidationSplitter(fold=1),\n",
    "                 item_tfms = [AudioNormalize], \n",
    "                 batch_tfms = [a2s, ChannelSpecNorm],\n",
    "                 get_y=ColReader(\"category\"))\n",
    "dbunch = auds.dataloaders(df, bs=64)\n",
    "\n",
    "\n",
    "accuracies = []\n",
    "for i in range(num_runs):\n",
    "    # make cnn learner\n",
    "    learn = cnn_learner(dbunch, \n",
    "                        resnet18, \n",
    "                        config=cnn_config(n_in=1),\n",
    "                        loss_fn=CrossEntropyLossFlat,\n",
    "                        metrics=[accuracy])\n",
    "    # fit one cycle for given epochs\n",
    "    learn.fit_one_cycle(epochs)\n",
    "    accuracies.append(learn.recorder.values[-1][-1])\n",
    "\n",
    "print(f'Average accuracy for \"channel\" normalization: {sum(accuracies) / num_runs}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
